{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0m5KR34gmRH"
   },
   "source": [
    "!pip install transformers\n",
    "!pip install datasetsLet's GO!\n",
    "\n",
    "Required Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEzSRp7nges6",
    "outputId": "6cf92343-ecdb-48c2-a874-82dc87c058d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers==4.3.2 in ./miniconda3/lib/python3.8/site-packages (4.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (4.64.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (2022.6.2)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (21.3)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (1.21.4)\n",
      "Requirement already satisfied: sacremoses in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (0.0.53)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from transformers==4.3.2) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging->transformers==4.3.2) (3.0.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.3.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.3.2) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.3.2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.3.2) (2021.5.30)\n",
      "Requirement already satisfied: six in ./miniconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.3.2) (1.16.0)\n",
      "Requirement already satisfied: click in ./miniconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.3.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./miniconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.3.2) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in ./miniconda3/lib/python3.8/site-packages (2.3.2)\n",
      "Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./miniconda3/lib/python3.8/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in ./miniconda3/lib/python3.8/site-packages (from datasets) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./miniconda3/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: multiprocess in ./miniconda3/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./miniconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: responses<0.19 in ./miniconda3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in ./miniconda3/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: xxhash in ./miniconda3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./miniconda3/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.3.2\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UIqpm34x2rms"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install sentencepiece\n",
    "\n",
    "##Set random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DLlMrhvnges7"
   },
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "28ddac0fc57f40a893c6c25d363d3b78",
      "7bdad124edbf4f149f803e01b3cb45ff",
      "023e75b78bc0458f812801638946d7ca",
      "f3777bf57258424c9bae0f180e38fca9",
      "aa248bd910fe47c18fe15bec280d2806",
      "330d42120384471192fcb9afaaa49e4f",
      "8d9d0143cb304eb596971adcc7e392e8",
      "458b776fbffd404eb69aab9d07f9be53",
      "088e74d10cd34b36b3743c5bac1e73fa",
      "1d3249030cc941c0933fca6aeacccb23",
      "f492bb406b4a455192e5348345e576d1"
     ]
    },
    "id": "youbtoYN4yWW",
    "outputId": "0249012f-cbb8-4e68-a1bb-02cff7c27caf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_review_full (/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a2557c9d0f4f5991058b0420b8dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXS24ico5-6Q",
    "outputId": "bace06d3-46d9-488b-aac5-2a8fb3b96ede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 4,\n",
       " 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iUXz1fMq8r1h"
   },
   "outputs": [],
   "source": [
    "train_num=len(dataset['train'])\n",
    "test_num=len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpdbsAMYgS1L",
    "outputId": "da151aaf-f253-4a4e-e9d6-8ca574b6e029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(train_num)\n",
    "print(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9XbIgB0lraxv"
   },
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "  data_dict={}\n",
    "  for i in range(len(data)):\n",
    "    data_dict[data[i]['text']]=data[i]['label']\n",
    "  return data_dict\n",
    "train_dict=get_dict(dataset['train'])\n",
    "test_dict=get_dict(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jYLLW8ZZ8OO1"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(train_dict.items()),columns = ['review_body','stars']) \n",
    "test_df  = pd.DataFrame(list(test_dict.items()),columns = ['review_body','stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "4dxEiYr6rAW5",
    "outputId": "ed68f1c8-41e7-4d0c-ed4f-02283500a245"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0           130000\n",
       "1           130000\n",
       "2           130000\n",
       "3           130000\n",
       "4           130000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['stars']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "jiGvSvpprAdB",
    "outputId": "e7b93b05-d574-494f-eab6-7867f7067702"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0            10000\n",
       "1            10000\n",
       "2            10000\n",
       "3            10000\n",
       "4            10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby(['stars']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsxZBggQreC5",
    "outputId": "be6e719a-73c6-423c-9e81-405f1633bdf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0eJtptXiGorl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "label_list = ['UNK_UNK','0','1','2','3','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6rO5LFODHMLl"
   },
   "outputs": [],
   "source": [
    "# def get_label_unlabel(train_df):\n",
    "#   cv = StratifiedKFold(n_splits=20,shuffle = True,random_state=42)\n",
    "#   i=0\n",
    "#   unlabel_idx=[]\n",
    "#   label_idx=[]\n",
    "#   for train_idxs, test_idxs in cv.split(train_df,train_df['stars']):\n",
    "#     if i in [0,1,2,3]:\n",
    "#       label_idx=np.concatenate((label_idx,test_idxs), axis=None)\n",
    "#       i+=1\n",
    "#     elif i in range(4,400,2):\n",
    "#       unlabel_idx=np.concatenate((unlabel_idx,test_idxs), axis=None)\n",
    "#       i+=1\n",
    "#     else:\n",
    "#       pass\n",
    "#       i+=1\n",
    "#   return train_df.loc[label_idx,:],train_df.loc[unlabel_idx,:]\n",
    "\n",
    "# def get_test_data(test_df):\n",
    "#   cv = StratifiedKFold(n_splits=5,shuffle = True,random_state=42)\n",
    "#   i=0\n",
    "#   for train_idxs, test_idxs in cv.split(test_df,test_df['stars']):\n",
    "#     if i==0:\n",
    "#       label_data=test_df.loc[test_idxs,:]\n",
    "#   return label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk4Kktsuikve"
   },
   "source": [
    "## Decrease train size to 200000 and test size to 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oiu-oVOgikIr"
   },
   "outputs": [],
   "source": [
    "def decrease_train(train_df,n_fold):\n",
    "  cv = StratifiedKFold(n_splits=n_fold,shuffle = True,random_state=42)\n",
    "  i=0\n",
    "  select_idx=[]\n",
    "  for train_idxs, test_idxs in cv.split(train_df,train_df['stars']):\n",
    "    if i in [i for i in range(0,3)]:\n",
    "      select_idx=np.concatenate((select_idx,test_idxs), axis=None)\n",
    "      i+=1\n",
    "    # elif i in range(4,400,2):\n",
    "    #   unlabel_idx=np.concatenate((unlabel_idx,test_idxs), axis=None)\n",
    "    #   i+=1\n",
    "    else:\n",
    "      i+=1\n",
    "  return train_df.loc[select_idx,:]\n",
    "train_example=decrease_train(train_df,10)\n",
    "\n",
    "\n",
    "def decrease_test(train_df,n_fold):\n",
    "  cv = StratifiedKFold(n_splits=n_fold,shuffle = True,random_state=42)\n",
    "  i=0\n",
    "  select_idx=[]\n",
    "  for train_idxs, test_idxs in cv.split(train_df,train_df['stars']):\n",
    "    if i in [i for i in range(0,100,10)]:\n",
    "      select_idx=np.concatenate((select_idx,test_idxs), axis=None)\n",
    "      i+=1\n",
    "    # elif i in range(4,400,2):\n",
    "    #   unlabel_idx=np.concatenate((unlabel_idx,test_idxs), axis=None)\n",
    "    #   i+=1\n",
    "    else:\n",
    "      i+=1\n",
    "  return train_df.loc[select_idx,:]\n",
    "test_example=decrease_test(test_df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "9Qx4ivrWjlFM",
    "outputId": "b04a351b-d88b-4b47-e35d-07340da47d5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0             1000\n",
       "1             1000\n",
       "2             1000\n",
       "3             1000\n",
       "4             1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example.groupby(['stars']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kKHSFFMVksuG"
   },
   "outputs": [],
   "source": [
    "train_example.reset_index(drop=True,inplace=True)\n",
    "test_example.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NuKVSIqges_"
   },
   "source": [
    "## Use 5% label data, the rest as unlabel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7nn5uoPnges_"
   },
   "outputs": [],
   "source": [
    "def get_label_unlabel(train_df):\n",
    "  cv = StratifiedKFold(n_splits=100,shuffle = True,random_state=42)\n",
    "  i=0\n",
    "  unlabel_idx=[]\n",
    "  label_idx=[]\n",
    "  for train_idxs, test_idxs in cv.split(train_df,train_df['stars']):\n",
    "    if i in [i for i in range(0,100,20)]:\n",
    "      label_idx=np.concatenate((label_idx,test_idxs), axis=None)\n",
    "      i+=1\n",
    "    # elif i in range(4,400,2):\n",
    "    #   unlabel_idx=np.concatenate((unlabel_idx,test_idxs), axis=None)\n",
    "    #   i+=1\n",
    "    else:\n",
    "      unlabel_idx=np.concatenate((unlabel_idx,test_idxs), axis=None)\n",
    "      i+=1\n",
    "  return train_df.loc[label_idx,:],train_df.loc[unlabel_idx,:]\n",
    "\n",
    "def get_test_data(test_df):\n",
    "  cv = StratifiedKFold(n_splits=5,shuffle = True,random_state=42)\n",
    "  i=0\n",
    "  for train_idxs, test_idxs in cv.split(test_df,test_df['stars']):\n",
    "    if i==0:\n",
    "      label_data=test_df.loc[test_idxs,:]\n",
    "  return label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1VU01e1YJVYS"
   },
   "outputs": [],
   "source": [
    "label_data,unlabel_data=get_label_unlabel(train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BMgz1FHqZE8u"
   },
   "outputs": [],
   "source": [
    "# test_data=get_test_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J1LhF1Xf8iHd"
   },
   "outputs": [],
   "source": [
    "test_data=test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdPtDGlzgEfm",
    "outputId": "c4aa40e3-c578-4f06-8d21-346fc2557c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_data： 9750\n",
      "unlabel_data： 185250\n",
      "test_data： 5000\n"
     ]
    }
   ],
   "source": [
    "print('label_data：',len(label_data))\n",
    "print('unlabel_data：',len(unlabel_data))\n",
    "print('test_data：',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "xCw8Pn47yyLd",
    "outputId": "08154974-8450-4817-d98d-feebed7f6b61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0             1950\n",
       "1             1950\n",
       "2             1950\n",
       "3             1950\n",
       "4             1950"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data.groupby('stars').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "gk6dyLYCy8Pj",
    "outputId": "923a162a-330f-40c8-9f3f-9670a1a09de2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0            37050\n",
       "1            37050\n",
       "2            37050\n",
       "3            37050\n",
       "4            37050"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabel_data.groupby('stars').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "-TrSnxHby9En",
    "outputId": "fb30e100-d74a-46d7-967a-cf22b9035158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_body\n",
       "stars             \n",
       "0             1000\n",
       "1             1000\n",
       "2             1000\n",
       "3             1000\n",
       "4             1000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.groupby('stars').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QWY-IXfXbR7e"
   },
   "outputs": [],
   "source": [
    "label_data.reset_index(drop=True,inplace=True)\n",
    "unlabel_data.reset_index(drop=True,inplace=True)\n",
    "test_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "JQEdhQtTY6HD"
   },
   "outputs": [],
   "source": [
    "def get_qc_examples(data,label=False):\n",
    "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "  examples = []\n",
    "  if label==True:\n",
    "    for i in range(len(data)):\n",
    "      x=data.loc[i,'review_body']\n",
    "      y=data.loc[i,'stars']\n",
    "      examples.append((x,y))\n",
    "  else:\n",
    "    for i in range(len(data)):\n",
    "      x=data.loc[i,'review_body']\n",
    "      y='UNK_UNK'\n",
    "      examples.append((x,y))\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gCpWSj4BY6MS"
   },
   "outputs": [],
   "source": [
    "#Load the examples\n",
    "labeled_examples = get_qc_examples(label_data,label=True)\n",
    "unlabeled_examples = get_qc_examples(unlabel_data)\n",
    "test_examples = get_qc_examples(test_data,label=True)\n",
    "\n",
    "# original size\n",
    "# train 688  \n",
    "# unlabel 14433\n",
    "# test 972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeZgRup520II",
    "outputId": "ee280f52-89ad-40a6-d19b-46f821c96b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU3ns8Ic7I-h"
   },
   "source": [
    "### Input Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jw0HC_hU3FUy"
   },
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size = 64\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 5\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "# model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WunkB3ifqy09"
   },
   "outputs": [],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "1a07fdd1e59a4fadb0e5ae24ddc5aa58",
      "de4ef3bf5467485fb91e13fb492f5e4a",
      "e9ac3edb2fa84247993d39b12b66d0a9",
      "3faab90fa4ad4daaa6c15214ee6c8648",
      "577e5d921add48f39e230d6e20afd9ac",
      "576590ab88514315a8c698fe178a84b5",
      "37be672361f94deda5d94b093a7c4a77",
      "c98d570667a2487da8e9c88955852c22",
      "158a2f31483a4f668ee1aad205c4d6ac",
      "d2395d83dc22422b8643f9c0e8fc741b",
      "b1ca3fa910bb40db86094c1cd78bb273",
      "4f6d01f1fb4d4631a91358f47f5b1507",
      "b1d0181e365642debacc42961d6f70c8",
      "bcbd78b9686c425a8eee96fa628bf752",
      "57deaebe133c4fee9e46f9e31ff739d0",
      "6c0c45edfef146fca0eec9f700a0f638",
      "bc8724ca9e9a484197a90f47fc3999d2",
      "1099803863ad4202a21fa5731cbccc17",
      "fb45fb4430074b03b02e2608ba56c24d",
      "841a888184524f1889c6228a5111150f",
      "af8962d0b3bf4a54ae27b388c8d7d708",
      "517dc9f1c1aa47ada04e459a3361a86d",
      "3a278779bf114a5d9c01b46d8e3a7084",
      "c5786bed9383410680a176c870145a36",
      "06e9ed4ad90f46a080bedde4f270269a",
      "5a9cf9a79f904592a328a77ab573c8e0",
      "3a84d2e1c7ce4d09b4ec694989469a70",
      "fd79f3c885ba47d59c952428bf3541d7",
      "bb156789ac354f7ca2e463302e81609e",
      "fef9a563ac1347b393985eb69cf8f634",
      "1f56206a1bc74620903cea2230e6ca54",
      "96176dc8861849edb248dd6b3a451955",
      "b57fd3bce49f4845addea1832dca1676"
     ]
    },
    "id": "U1BHtvmkmGXO",
    "outputId": "775c75a1-d6e1-4182-976e-28a190ca5960"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25621dee41d847ba9e862daaedea0a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078a52dc47fe4a4d91a1956225e2b37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256ee2398d6f4235a8d8c88154f56e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length: 256\n"
     ]
    }
   ],
   "source": [
    "lengths_dict = {}\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# get train_label length\n",
    "lengths = []\n",
    "tk0 = tqdm(labeled_examples, total=len(labeled_examples))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text[0], add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "lengths_dict['label'] = lengths\n",
    "\n",
    "# get train_unlabel length\n",
    "lengths = []\n",
    "tk0 = tqdm(unlabeled_examples, total=len(unlabeled_examples))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text[0], add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "lengths_dict['unlabel'] = lengths\n",
    "\n",
    "# get test length\n",
    "lengths = []\n",
    "tk0 = tqdm(test_examples, total=len(test_examples))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text[0], add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "lengths_dict['test'] = lengths\n",
    "    \n",
    "max_len = max(max(lengths_dict['label']),max(lengths_dict['test']),max(lengths_dict['unlabel'])) + 2 # CLS + SEP \n",
    "if max_len>256:\n",
    "  max_seq_length=256\n",
    "else:\n",
    "  max_seq_length = max_len\n",
    "print(f\"max_seq_length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBhaW5vBfR6B"
   },
   "source": [
    "Functions required to convert examples into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fmKL5AD7I4Zg"
   },
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[str(text[1])])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do3O-VeefT3g"
   },
   "source": [
    "Convert the input examples into DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4c-nsMXlKX-D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3966/3216393040.py:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  label_mask_array = torch.tensor(label_mask_array)\n"
     ]
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ihcw3vquaQm"
   },
   "source": [
    "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "18kY64-n3I6y"
   },
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uje9s2zQunFc"
   },
   "source": [
    "We instantiate the Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Ylz5rvqE3U2S"
   },
   "outputs": [],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG3qzp2-usZE"
   },
   "source": [
    "Let's go with the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NhqylHGK3Va4",
    "outputId": "8115deb3-9379-4196-c8f5-2b4a4c272204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of  3,504.    Elapsed: 0:00:07.\n",
      "  Batch    20  of  3,504.    Elapsed: 0:00:14.\n",
      "  Batch    30  of  3,504.    Elapsed: 0:00:21.\n",
      "  Batch    40  of  3,504.    Elapsed: 0:00:27.\n",
      "  Batch    50  of  3,504.    Elapsed: 0:00:34.\n",
      "  Batch    60  of  3,504.    Elapsed: 0:00:41.\n",
      "  Batch    70  of  3,504.    Elapsed: 0:00:48.\n",
      "  Batch    80  of  3,504.    Elapsed: 0:00:55.\n",
      "  Batch    90  of  3,504.    Elapsed: 0:01:02.\n",
      "  Batch   100  of  3,504.    Elapsed: 0:01:09.\n",
      "  Batch   110  of  3,504.    Elapsed: 0:01:16.\n",
      "  Batch   120  of  3,504.    Elapsed: 0:01:23.\n",
      "  Batch   130  of  3,504.    Elapsed: 0:01:30.\n",
      "  Batch   140  of  3,504.    Elapsed: 0:01:37.\n",
      "  Batch   150  of  3,504.    Elapsed: 0:01:44.\n",
      "  Batch   160  of  3,504.    Elapsed: 0:01:51.\n",
      "  Batch   170  of  3,504.    Elapsed: 0:01:57.\n",
      "  Batch   180  of  3,504.    Elapsed: 0:02:04.\n",
      "  Batch   190  of  3,504.    Elapsed: 0:02:11.\n",
      "  Batch   200  of  3,504.    Elapsed: 0:02:18.\n",
      "  Batch   210  of  3,504.    Elapsed: 0:02:25.\n",
      "  Batch   220  of  3,504.    Elapsed: 0:02:32.\n",
      "  Batch   230  of  3,504.    Elapsed: 0:02:39.\n",
      "  Batch   240  of  3,504.    Elapsed: 0:02:46.\n",
      "  Batch   250  of  3,504.    Elapsed: 0:02:53.\n",
      "  Batch   260  of  3,504.    Elapsed: 0:03:00.\n",
      "  Batch   270  of  3,504.    Elapsed: 0:03:07.\n",
      "  Batch   280  of  3,504.    Elapsed: 0:03:14.\n",
      "  Batch   290  of  3,504.    Elapsed: 0:03:21.\n",
      "  Batch   300  of  3,504.    Elapsed: 0:03:28.\n",
      "  Batch   310  of  3,504.    Elapsed: 0:03:35.\n",
      "  Batch   320  of  3,504.    Elapsed: 0:03:42.\n",
      "  Batch   330  of  3,504.    Elapsed: 0:03:49.\n",
      "  Batch   340  of  3,504.    Elapsed: 0:03:56.\n",
      "  Batch   350  of  3,504.    Elapsed: 0:04:03.\n",
      "  Batch   360  of  3,504.    Elapsed: 0:04:10.\n",
      "  Batch   370  of  3,504.    Elapsed: 0:04:17.\n",
      "  Batch   380  of  3,504.    Elapsed: 0:04:24.\n",
      "  Batch   390  of  3,504.    Elapsed: 0:04:31.\n",
      "  Batch   400  of  3,504.    Elapsed: 0:04:37.\n",
      "  Batch   410  of  3,504.    Elapsed: 0:04:44.\n",
      "  Batch   420  of  3,504.    Elapsed: 0:04:51.\n",
      "  Batch   430  of  3,504.    Elapsed: 0:04:58.\n",
      "  Batch   440  of  3,504.    Elapsed: 0:05:05.\n",
      "  Batch   450  of  3,504.    Elapsed: 0:05:12.\n",
      "  Batch   460  of  3,504.    Elapsed: 0:05:19.\n",
      "  Batch   470  of  3,504.    Elapsed: 0:05:26.\n",
      "  Batch   480  of  3,504.    Elapsed: 0:05:33.\n",
      "  Batch   490  of  3,504.    Elapsed: 0:05:40.\n",
      "  Batch   500  of  3,504.    Elapsed: 0:05:47.\n",
      "  Batch   510  of  3,504.    Elapsed: 0:05:54.\n",
      "  Batch   520  of  3,504.    Elapsed: 0:06:01.\n",
      "  Batch   530  of  3,504.    Elapsed: 0:06:08.\n",
      "  Batch   540  of  3,504.    Elapsed: 0:06:15.\n",
      "  Batch   550  of  3,504.    Elapsed: 0:06:22.\n",
      "  Batch   560  of  3,504.    Elapsed: 0:06:29.\n",
      "  Batch   570  of  3,504.    Elapsed: 0:06:36.\n",
      "  Batch   580  of  3,504.    Elapsed: 0:06:43.\n",
      "  Batch   590  of  3,504.    Elapsed: 0:06:50.\n",
      "  Batch   600  of  3,504.    Elapsed: 0:06:57.\n",
      "  Batch   610  of  3,504.    Elapsed: 0:07:04.\n",
      "  Batch   620  of  3,504.    Elapsed: 0:07:11.\n",
      "  Batch   630  of  3,504.    Elapsed: 0:07:18.\n",
      "  Batch   640  of  3,504.    Elapsed: 0:07:25.\n",
      "  Batch   650  of  3,504.    Elapsed: 0:07:32.\n",
      "  Batch   660  of  3,504.    Elapsed: 0:07:39.\n",
      "  Batch   670  of  3,504.    Elapsed: 0:07:46.\n",
      "  Batch   680  of  3,504.    Elapsed: 0:07:53.\n",
      "  Batch   690  of  3,504.    Elapsed: 0:08:00.\n",
      "  Batch   700  of  3,504.    Elapsed: 0:08:07.\n",
      "  Batch   710  of  3,504.    Elapsed: 0:08:14.\n",
      "  Batch   720  of  3,504.    Elapsed: 0:08:20.\n",
      "  Batch   730  of  3,504.    Elapsed: 0:08:27.\n",
      "  Batch   740  of  3,504.    Elapsed: 0:08:34.\n",
      "  Batch   750  of  3,504.    Elapsed: 0:08:41.\n",
      "  Batch   760  of  3,504.    Elapsed: 0:08:48.\n",
      "  Batch   770  of  3,504.    Elapsed: 0:08:55.\n",
      "  Batch   780  of  3,504.    Elapsed: 0:09:02.\n",
      "  Batch   790  of  3,504.    Elapsed: 0:09:09.\n",
      "  Batch   800  of  3,504.    Elapsed: 0:09:16.\n",
      "  Batch   810  of  3,504.    Elapsed: 0:09:23.\n",
      "  Batch   820  of  3,504.    Elapsed: 0:09:30.\n",
      "  Batch   830  of  3,504.    Elapsed: 0:09:37.\n",
      "  Batch   840  of  3,504.    Elapsed: 0:09:44.\n",
      "  Batch   850  of  3,504.    Elapsed: 0:09:51.\n",
      "  Batch   860  of  3,504.    Elapsed: 0:09:58.\n",
      "  Batch   870  of  3,504.    Elapsed: 0:10:05.\n",
      "  Batch   880  of  3,504.    Elapsed: 0:10:12.\n",
      "  Batch   890  of  3,504.    Elapsed: 0:10:19.\n",
      "  Batch   900  of  3,504.    Elapsed: 0:10:26.\n",
      "  Batch   910  of  3,504.    Elapsed: 0:10:33.\n",
      "  Batch   920  of  3,504.    Elapsed: 0:10:40.\n",
      "  Batch   930  of  3,504.    Elapsed: 0:10:47.\n",
      "  Batch   940  of  3,504.    Elapsed: 0:10:54.\n",
      "  Batch   950  of  3,504.    Elapsed: 0:11:01.\n",
      "  Batch   960  of  3,504.    Elapsed: 0:11:08.\n",
      "  Batch   970  of  3,504.    Elapsed: 0:11:15.\n",
      "  Batch   980  of  3,504.    Elapsed: 0:11:22.\n",
      "  Batch   990  of  3,504.    Elapsed: 0:11:29.\n",
      "  Batch 1,000  of  3,504.    Elapsed: 0:11:36.\n",
      "  Batch 1,010  of  3,504.    Elapsed: 0:11:43.\n",
      "  Batch 1,020  of  3,504.    Elapsed: 0:11:49.\n",
      "  Batch 1,030  of  3,504.    Elapsed: 0:11:56.\n",
      "  Batch 1,040  of  3,504.    Elapsed: 0:12:03.\n",
      "  Batch 1,050  of  3,504.    Elapsed: 0:12:10.\n",
      "  Batch 1,060  of  3,504.    Elapsed: 0:12:17.\n",
      "  Batch 1,070  of  3,504.    Elapsed: 0:12:24.\n",
      "  Batch 1,080  of  3,504.    Elapsed: 0:12:31.\n",
      "  Batch 1,090  of  3,504.    Elapsed: 0:12:38.\n",
      "  Batch 1,100  of  3,504.    Elapsed: 0:12:45.\n",
      "  Batch 1,110  of  3,504.    Elapsed: 0:12:52.\n",
      "  Batch 1,120  of  3,504.    Elapsed: 0:12:59.\n",
      "  Batch 1,130  of  3,504.    Elapsed: 0:13:06.\n",
      "  Batch 1,140  of  3,504.    Elapsed: 0:13:13.\n",
      "  Batch 1,150  of  3,504.    Elapsed: 0:13:20.\n",
      "  Batch 1,160  of  3,504.    Elapsed: 0:13:27.\n",
      "  Batch 1,170  of  3,504.    Elapsed: 0:13:34.\n",
      "  Batch 1,180  of  3,504.    Elapsed: 0:13:41.\n",
      "  Batch 1,190  of  3,504.    Elapsed: 0:13:48.\n",
      "  Batch 1,200  of  3,504.    Elapsed: 0:13:55.\n",
      "  Batch 1,210  of  3,504.    Elapsed: 0:14:02.\n",
      "  Batch 1,220  of  3,504.    Elapsed: 0:14:09.\n",
      "  Batch 1,230  of  3,504.    Elapsed: 0:14:16.\n",
      "  Batch 1,240  of  3,504.    Elapsed: 0:14:23.\n",
      "  Batch 1,250  of  3,504.    Elapsed: 0:14:30.\n",
      "  Batch 1,260  of  3,504.    Elapsed: 0:14:37.\n",
      "  Batch 1,270  of  3,504.    Elapsed: 0:14:44.\n",
      "  Batch 1,280  of  3,504.    Elapsed: 0:14:51.\n",
      "  Batch 1,290  of  3,504.    Elapsed: 0:14:58.\n",
      "  Batch 1,300  of  3,504.    Elapsed: 0:15:05.\n",
      "  Batch 1,310  of  3,504.    Elapsed: 0:15:12.\n",
      "  Batch 1,320  of  3,504.    Elapsed: 0:15:18.\n",
      "  Batch 1,330  of  3,504.    Elapsed: 0:15:25.\n",
      "  Batch 1,340  of  3,504.    Elapsed: 0:15:32.\n",
      "  Batch 1,350  of  3,504.    Elapsed: 0:15:39.\n",
      "  Batch 1,360  of  3,504.    Elapsed: 0:15:46.\n",
      "  Batch 1,370  of  3,504.    Elapsed: 0:15:53.\n",
      "  Batch 1,380  of  3,504.    Elapsed: 0:16:00.\n",
      "  Batch 1,390  of  3,504.    Elapsed: 0:16:07.\n",
      "  Batch 1,400  of  3,504.    Elapsed: 0:16:14.\n",
      "  Batch 1,410  of  3,504.    Elapsed: 0:16:21.\n",
      "  Batch 1,420  of  3,504.    Elapsed: 0:16:28.\n",
      "  Batch 1,430  of  3,504.    Elapsed: 0:16:35.\n",
      "  Batch 1,440  of  3,504.    Elapsed: 0:16:42.\n",
      "  Batch 1,450  of  3,504.    Elapsed: 0:16:49.\n",
      "  Batch 1,460  of  3,504.    Elapsed: 0:16:56.\n",
      "  Batch 1,470  of  3,504.    Elapsed: 0:17:03.\n",
      "  Batch 1,480  of  3,504.    Elapsed: 0:17:10.\n",
      "  Batch 1,490  of  3,504.    Elapsed: 0:17:17.\n",
      "  Batch 1,500  of  3,504.    Elapsed: 0:17:24.\n",
      "  Batch 1,510  of  3,504.    Elapsed: 0:17:31.\n",
      "  Batch 1,520  of  3,504.    Elapsed: 0:17:38.\n",
      "  Batch 1,530  of  3,504.    Elapsed: 0:17:45.\n",
      "  Batch 1,540  of  3,504.    Elapsed: 0:17:52.\n",
      "  Batch 1,550  of  3,504.    Elapsed: 0:17:59.\n",
      "  Batch 1,560  of  3,504.    Elapsed: 0:18:06.\n",
      "  Batch 1,570  of  3,504.    Elapsed: 0:18:13.\n",
      "  Batch 1,580  of  3,504.    Elapsed: 0:18:20.\n",
      "  Batch 1,590  of  3,504.    Elapsed: 0:18:27.\n",
      "  Batch 1,600  of  3,504.    Elapsed: 0:18:34.\n",
      "  Batch 1,610  of  3,504.    Elapsed: 0:18:41.\n",
      "  Batch 1,620  of  3,504.    Elapsed: 0:18:48.\n",
      "  Batch 1,630  of  3,504.    Elapsed: 0:18:55.\n",
      "  Batch 1,640  of  3,504.    Elapsed: 0:19:02.\n",
      "  Batch 1,650  of  3,504.    Elapsed: 0:19:09.\n",
      "  Batch 1,660  of  3,504.    Elapsed: 0:19:15.\n",
      "  Batch 1,670  of  3,504.    Elapsed: 0:19:22.\n",
      "  Batch 1,680  of  3,504.    Elapsed: 0:19:29.\n",
      "  Batch 1,690  of  3,504.    Elapsed: 0:19:36.\n",
      "  Batch 1,700  of  3,504.    Elapsed: 0:19:43.\n",
      "  Batch 1,710  of  3,504.    Elapsed: 0:19:50.\n",
      "  Batch 1,720  of  3,504.    Elapsed: 0:19:57.\n",
      "  Batch 1,730  of  3,504.    Elapsed: 0:20:04.\n",
      "  Batch 1,740  of  3,504.    Elapsed: 0:20:11.\n",
      "  Batch 1,750  of  3,504.    Elapsed: 0:20:18.\n",
      "  Batch 1,760  of  3,504.    Elapsed: 0:20:25.\n",
      "  Batch 1,770  of  3,504.    Elapsed: 0:20:32.\n",
      "  Batch 1,780  of  3,504.    Elapsed: 0:20:39.\n",
      "  Batch 1,790  of  3,504.    Elapsed: 0:20:46.\n",
      "  Batch 1,800  of  3,504.    Elapsed: 0:20:53.\n",
      "  Batch 1,810  of  3,504.    Elapsed: 0:21:00.\n",
      "  Batch 1,820  of  3,504.    Elapsed: 0:21:07.\n",
      "  Batch 1,830  of  3,504.    Elapsed: 0:21:14.\n",
      "  Batch 1,840  of  3,504.    Elapsed: 0:21:21.\n",
      "  Batch 1,850  of  3,504.    Elapsed: 0:21:28.\n",
      "  Batch 1,860  of  3,504.    Elapsed: 0:21:35.\n",
      "  Batch 1,870  of  3,504.    Elapsed: 0:21:42.\n",
      "  Batch 1,880  of  3,504.    Elapsed: 0:21:49.\n",
      "  Batch 1,890  of  3,504.    Elapsed: 0:21:55.\n",
      "  Batch 1,900  of  3,504.    Elapsed: 0:22:02.\n",
      "  Batch 1,910  of  3,504.    Elapsed: 0:22:09.\n",
      "  Batch 1,920  of  3,504.    Elapsed: 0:22:16.\n",
      "  Batch 1,930  of  3,504.    Elapsed: 0:22:23.\n",
      "  Batch 1,940  of  3,504.    Elapsed: 0:22:30.\n",
      "  Batch 1,950  of  3,504.    Elapsed: 0:22:37.\n",
      "  Batch 1,960  of  3,504.    Elapsed: 0:22:44.\n",
      "  Batch 1,970  of  3,504.    Elapsed: 0:22:51.\n",
      "  Batch 1,980  of  3,504.    Elapsed: 0:22:58.\n",
      "  Batch 1,990  of  3,504.    Elapsed: 0:23:05.\n",
      "  Batch 2,000  of  3,504.    Elapsed: 0:23:12.\n",
      "  Batch 2,010  of  3,504.    Elapsed: 0:23:19.\n",
      "  Batch 2,020  of  3,504.    Elapsed: 0:23:26.\n",
      "  Batch 2,030  of  3,504.    Elapsed: 0:23:33.\n",
      "  Batch 2,040  of  3,504.    Elapsed: 0:23:40.\n",
      "  Batch 2,050  of  3,504.    Elapsed: 0:23:47.\n",
      "  Batch 2,060  of  3,504.    Elapsed: 0:23:54.\n",
      "  Batch 2,070  of  3,504.    Elapsed: 0:24:01.\n",
      "  Batch 2,080  of  3,504.    Elapsed: 0:24:08.\n",
      "  Batch 2,090  of  3,504.    Elapsed: 0:24:15.\n",
      "  Batch 2,100  of  3,504.    Elapsed: 0:24:22.\n",
      "  Batch 2,110  of  3,504.    Elapsed: 0:24:29.\n",
      "  Batch 2,120  of  3,504.    Elapsed: 0:24:36.\n",
      "  Batch 2,130  of  3,504.    Elapsed: 0:24:43.\n",
      "  Batch 2,140  of  3,504.    Elapsed: 0:24:50.\n",
      "  Batch 2,150  of  3,504.    Elapsed: 0:24:57.\n",
      "  Batch 2,160  of  3,504.    Elapsed: 0:25:04.\n",
      "  Batch 2,170  of  3,504.    Elapsed: 0:25:11.\n",
      "  Batch 2,180  of  3,504.    Elapsed: 0:25:18.\n",
      "  Batch 2,190  of  3,504.    Elapsed: 0:25:25.\n",
      "  Batch 2,200  of  3,504.    Elapsed: 0:25:32.\n",
      "  Batch 2,210  of  3,504.    Elapsed: 0:25:39.\n",
      "  Batch 2,220  of  3,504.    Elapsed: 0:25:46.\n",
      "  Batch 2,230  of  3,504.    Elapsed: 0:25:52.\n",
      "  Batch 2,240  of  3,504.    Elapsed: 0:25:59.\n",
      "  Batch 2,250  of  3,504.    Elapsed: 0:26:06.\n",
      "  Batch 2,260  of  3,504.    Elapsed: 0:26:13.\n",
      "  Batch 2,270  of  3,504.    Elapsed: 0:26:20.\n",
      "  Batch 2,280  of  3,504.    Elapsed: 0:26:27.\n",
      "  Batch 2,290  of  3,504.    Elapsed: 0:26:34.\n",
      "  Batch 2,300  of  3,504.    Elapsed: 0:26:41.\n",
      "  Batch 2,310  of  3,504.    Elapsed: 0:26:48.\n",
      "  Batch 2,320  of  3,504.    Elapsed: 0:26:55.\n",
      "  Batch 2,330  of  3,504.    Elapsed: 0:27:02.\n",
      "  Batch 2,340  of  3,504.    Elapsed: 0:27:09.\n",
      "  Batch 2,350  of  3,504.    Elapsed: 0:27:16.\n",
      "  Batch 2,360  of  3,504.    Elapsed: 0:27:23.\n",
      "  Batch 2,370  of  3,504.    Elapsed: 0:27:30.\n",
      "  Batch 2,380  of  3,504.    Elapsed: 0:27:37.\n",
      "  Batch 2,390  of  3,504.    Elapsed: 0:27:44.\n",
      "  Batch 2,400  of  3,504.    Elapsed: 0:27:51.\n",
      "  Batch 2,410  of  3,504.    Elapsed: 0:27:58.\n",
      "  Batch 2,420  of  3,504.    Elapsed: 0:28:05.\n",
      "  Batch 2,430  of  3,504.    Elapsed: 0:28:12.\n",
      "  Batch 2,440  of  3,504.    Elapsed: 0:28:19.\n",
      "  Batch 2,450  of  3,504.    Elapsed: 0:28:26.\n",
      "  Batch 2,460  of  3,504.    Elapsed: 0:28:33.\n",
      "  Batch 2,470  of  3,504.    Elapsed: 0:28:40.\n",
      "  Batch 2,480  of  3,504.    Elapsed: 0:28:47.\n",
      "  Batch 2,490  of  3,504.    Elapsed: 0:28:54.\n",
      "  Batch 2,500  of  3,504.    Elapsed: 0:29:01.\n",
      "  Batch 2,510  of  3,504.    Elapsed: 0:29:08.\n",
      "  Batch 2,520  of  3,504.    Elapsed: 0:29:14.\n",
      "  Batch 2,530  of  3,504.    Elapsed: 0:29:21.\n",
      "  Batch 2,540  of  3,504.    Elapsed: 0:29:28.\n",
      "  Batch 2,550  of  3,504.    Elapsed: 0:29:35.\n",
      "  Batch 2,560  of  3,504.    Elapsed: 0:29:42.\n",
      "  Batch 2,570  of  3,504.    Elapsed: 0:29:49.\n",
      "  Batch 2,580  of  3,504.    Elapsed: 0:29:56.\n",
      "  Batch 2,590  of  3,504.    Elapsed: 0:30:03.\n",
      "  Batch 2,600  of  3,504.    Elapsed: 0:30:10.\n",
      "  Batch 2,610  of  3,504.    Elapsed: 0:30:17.\n",
      "  Batch 2,620  of  3,504.    Elapsed: 0:30:24.\n",
      "  Batch 2,630  of  3,504.    Elapsed: 0:30:31.\n",
      "  Batch 2,640  of  3,504.    Elapsed: 0:30:38.\n",
      "  Batch 2,650  of  3,504.    Elapsed: 0:30:45.\n",
      "  Batch 2,660  of  3,504.    Elapsed: 0:30:52.\n",
      "  Batch 2,670  of  3,504.    Elapsed: 0:30:59.\n",
      "  Batch 2,680  of  3,504.    Elapsed: 0:31:06.\n",
      "  Batch 2,690  of  3,504.    Elapsed: 0:31:13.\n",
      "  Batch 2,700  of  3,504.    Elapsed: 0:31:20.\n",
      "  Batch 2,710  of  3,504.    Elapsed: 0:31:27.\n",
      "  Batch 2,720  of  3,504.    Elapsed: 0:31:34.\n",
      "  Batch 2,730  of  3,504.    Elapsed: 0:31:41.\n",
      "  Batch 2,740  of  3,504.    Elapsed: 0:31:48.\n",
      "  Batch 2,750  of  3,504.    Elapsed: 0:31:55.\n",
      "  Batch 2,760  of  3,504.    Elapsed: 0:32:02.\n",
      "  Batch 2,770  of  3,504.    Elapsed: 0:32:09.\n",
      "  Batch 2,780  of  3,504.    Elapsed: 0:32:16.\n",
      "  Batch 2,790  of  3,504.    Elapsed: 0:32:23.\n",
      "  Batch 2,800  of  3,504.    Elapsed: 0:32:29.\n",
      "  Batch 2,810  of  3,504.    Elapsed: 0:32:36.\n",
      "  Batch 2,820  of  3,504.    Elapsed: 0:32:43.\n",
      "  Batch 2,830  of  3,504.    Elapsed: 0:32:50.\n",
      "  Batch 2,840  of  3,504.    Elapsed: 0:32:57.\n",
      "  Batch 2,850  of  3,504.    Elapsed: 0:33:04.\n",
      "  Batch 2,860  of  3,504.    Elapsed: 0:33:11.\n",
      "  Batch 2,870  of  3,504.    Elapsed: 0:33:18.\n",
      "  Batch 2,880  of  3,504.    Elapsed: 0:33:25.\n",
      "  Batch 2,890  of  3,504.    Elapsed: 0:33:32.\n",
      "  Batch 2,900  of  3,504.    Elapsed: 0:33:39.\n",
      "  Batch 2,910  of  3,504.    Elapsed: 0:33:46.\n",
      "  Batch 2,920  of  3,504.    Elapsed: 0:33:53.\n",
      "  Batch 2,930  of  3,504.    Elapsed: 0:34:00.\n",
      "  Batch 2,940  of  3,504.    Elapsed: 0:34:07.\n",
      "  Batch 2,950  of  3,504.    Elapsed: 0:34:14.\n",
      "  Batch 2,960  of  3,504.    Elapsed: 0:34:21.\n",
      "  Batch 2,970  of  3,504.    Elapsed: 0:34:28.\n",
      "  Batch 2,980  of  3,504.    Elapsed: 0:34:35.\n",
      "  Batch 2,990  of  3,504.    Elapsed: 0:34:42.\n",
      "  Batch 3,000  of  3,504.    Elapsed: 0:34:49.\n",
      "  Batch 3,010  of  3,504.    Elapsed: 0:34:56.\n",
      "  Batch 3,020  of  3,504.    Elapsed: 0:35:03.\n",
      "  Batch 3,030  of  3,504.    Elapsed: 0:35:10.\n",
      "  Batch 3,040  of  3,504.    Elapsed: 0:35:17.\n",
      "  Batch 3,050  of  3,504.    Elapsed: 0:35:24.\n",
      "  Batch 3,060  of  3,504.    Elapsed: 0:35:31.\n",
      "  Batch 3,070  of  3,504.    Elapsed: 0:35:38.\n",
      "  Batch 3,080  of  3,504.    Elapsed: 0:35:45.\n",
      "  Batch 3,090  of  3,504.    Elapsed: 0:35:52.\n",
      "  Batch 3,100  of  3,504.    Elapsed: 0:35:59.\n",
      "  Batch 3,110  of  3,504.    Elapsed: 0:36:06.\n",
      "  Batch 3,120  of  3,504.    Elapsed: 0:36:13.\n",
      "  Batch 3,130  of  3,504.    Elapsed: 0:36:19.\n",
      "  Batch 3,140  of  3,504.    Elapsed: 0:36:26.\n",
      "  Batch 3,150  of  3,504.    Elapsed: 0:36:33.\n",
      "  Batch 3,160  of  3,504.    Elapsed: 0:36:40.\n",
      "  Batch 3,170  of  3,504.    Elapsed: 0:36:47.\n",
      "  Batch 3,180  of  3,504.    Elapsed: 0:36:54.\n",
      "  Batch 3,190  of  3,504.    Elapsed: 0:37:01.\n",
      "  Batch 3,200  of  3,504.    Elapsed: 0:37:08.\n",
      "  Batch 3,210  of  3,504.    Elapsed: 0:37:15.\n",
      "  Batch 3,220  of  3,504.    Elapsed: 0:37:22.\n",
      "  Batch 3,230  of  3,504.    Elapsed: 0:37:29.\n",
      "  Batch 3,240  of  3,504.    Elapsed: 0:37:36.\n",
      "  Batch 3,250  of  3,504.    Elapsed: 0:37:43.\n",
      "  Batch 3,260  of  3,504.    Elapsed: 0:37:50.\n",
      "  Batch 3,270  of  3,504.    Elapsed: 0:37:57.\n",
      "  Batch 3,280  of  3,504.    Elapsed: 0:38:04.\n",
      "  Batch 3,290  of  3,504.    Elapsed: 0:38:11.\n",
      "  Batch 3,300  of  3,504.    Elapsed: 0:38:18.\n",
      "  Batch 3,310  of  3,504.    Elapsed: 0:38:25.\n",
      "  Batch 3,320  of  3,504.    Elapsed: 0:38:32.\n",
      "  Batch 3,330  of  3,504.    Elapsed: 0:38:39.\n",
      "  Batch 3,340  of  3,504.    Elapsed: 0:38:46.\n",
      "  Batch 3,350  of  3,504.    Elapsed: 0:38:53.\n",
      "  Batch 3,360  of  3,504.    Elapsed: 0:39:00.\n",
      "  Batch 3,370  of  3,504.    Elapsed: 0:39:07.\n",
      "  Batch 3,380  of  3,504.    Elapsed: 0:39:14.\n",
      "  Batch 3,390  of  3,504.    Elapsed: 0:39:21.\n",
      "  Batch 3,400  of  3,504.    Elapsed: 0:39:28.\n",
      "  Batch 3,410  of  3,504.    Elapsed: 0:39:35.\n",
      "  Batch 3,420  of  3,504.    Elapsed: 0:39:42.\n",
      "  Batch 3,430  of  3,504.    Elapsed: 0:39:49.\n",
      "  Batch 3,440  of  3,504.    Elapsed: 0:39:56.\n",
      "  Batch 3,450  of  3,504.    Elapsed: 0:40:03.\n",
      "  Batch 3,460  of  3,504.    Elapsed: 0:40:09.\n",
      "  Batch 3,470  of  3,504.    Elapsed: 0:40:16.\n",
      "  Batch 3,480  of  3,504.    Elapsed: 0:40:23.\n",
      "  Batch 3,490  of  3,504.    Elapsed: 0:40:30.\n",
      "  Batch 3,500  of  3,504.    Elapsed: 0:40:37.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 1.594\n",
      "  Training epcoh took: 0:40:40\n",
      "\n",
      "Running Test...\n",
      "all_preds\n",
      "[3 1 1 ... 3 4 4]\n",
      "all_labels\n",
      "[3 1 1 ... 3 5 4]\n",
      "  Accuracy: 0.557\n",
      "  Test Loss: 1.285\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of  3,504.    Elapsed: 0:00:07.\n",
      "  Batch    20  of  3,504.    Elapsed: 0:00:14.\n",
      "  Batch    30  of  3,504.    Elapsed: 0:00:21.\n",
      "  Batch    40  of  3,504.    Elapsed: 0:00:28.\n",
      "  Batch    50  of  3,504.    Elapsed: 0:00:35.\n",
      "  Batch    60  of  3,504.    Elapsed: 0:00:42.\n",
      "  Batch    70  of  3,504.    Elapsed: 0:00:49.\n",
      "  Batch    80  of  3,504.    Elapsed: 0:00:56.\n",
      "  Batch    90  of  3,504.    Elapsed: 0:01:03.\n",
      "  Batch   100  of  3,504.    Elapsed: 0:01:10.\n",
      "  Batch   110  of  3,504.    Elapsed: 0:01:17.\n",
      "  Batch   120  of  3,504.    Elapsed: 0:01:24.\n",
      "  Batch   130  of  3,504.    Elapsed: 0:01:31.\n",
      "  Batch   140  of  3,504.    Elapsed: 0:01:38.\n",
      "  Batch   150  of  3,504.    Elapsed: 0:01:45.\n",
      "  Batch   160  of  3,504.    Elapsed: 0:01:52.\n",
      "  Batch   170  of  3,504.    Elapsed: 0:01:59.\n",
      "  Batch   180  of  3,504.    Elapsed: 0:02:05.\n",
      "  Batch   190  of  3,504.    Elapsed: 0:02:12.\n",
      "  Batch   200  of  3,504.    Elapsed: 0:02:19.\n",
      "  Batch   210  of  3,504.    Elapsed: 0:02:26.\n",
      "  Batch   220  of  3,504.    Elapsed: 0:02:33.\n",
      "  Batch   230  of  3,504.    Elapsed: 0:02:40.\n",
      "  Batch   240  of  3,504.    Elapsed: 0:02:47.\n",
      "  Batch   250  of  3,504.    Elapsed: 0:02:54.\n",
      "  Batch   260  of  3,504.    Elapsed: 0:03:01.\n",
      "  Batch   270  of  3,504.    Elapsed: 0:03:08.\n",
      "  Batch   280  of  3,504.    Elapsed: 0:03:15.\n",
      "  Batch   290  of  3,504.    Elapsed: 0:03:22.\n",
      "  Batch   300  of  3,504.    Elapsed: 0:03:29.\n",
      "  Batch   310  of  3,504.    Elapsed: 0:03:36.\n",
      "  Batch   320  of  3,504.    Elapsed: 0:03:43.\n",
      "  Batch   330  of  3,504.    Elapsed: 0:03:50.\n",
      "  Batch   340  of  3,504.    Elapsed: 0:03:57.\n",
      "  Batch   350  of  3,504.    Elapsed: 0:04:04.\n",
      "  Batch   360  of  3,504.    Elapsed: 0:04:11.\n",
      "  Batch   370  of  3,504.    Elapsed: 0:04:18.\n",
      "  Batch   380  of  3,504.    Elapsed: 0:04:25.\n",
      "  Batch   390  of  3,504.    Elapsed: 0:04:32.\n",
      "  Batch   400  of  3,504.    Elapsed: 0:04:39.\n",
      "  Batch   410  of  3,504.    Elapsed: 0:04:46.\n",
      "  Batch   420  of  3,504.    Elapsed: 0:04:53.\n",
      "  Batch   430  of  3,504.    Elapsed: 0:05:00.\n",
      "  Batch   440  of  3,504.    Elapsed: 0:05:06.\n",
      "  Batch   450  of  3,504.    Elapsed: 0:05:13.\n",
      "  Batch   460  of  3,504.    Elapsed: 0:05:20.\n",
      "  Batch   470  of  3,504.    Elapsed: 0:05:27.\n",
      "  Batch   480  of  3,504.    Elapsed: 0:05:34.\n",
      "  Batch   490  of  3,504.    Elapsed: 0:05:41.\n",
      "  Batch   500  of  3,504.    Elapsed: 0:05:48.\n",
      "  Batch   510  of  3,504.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  3,504.    Elapsed: 0:06:02.\n",
      "  Batch   530  of  3,504.    Elapsed: 0:06:09.\n",
      "  Batch   540  of  3,504.    Elapsed: 0:06:16.\n",
      "  Batch   550  of  3,504.    Elapsed: 0:06:23.\n",
      "  Batch   560  of  3,504.    Elapsed: 0:06:30.\n",
      "  Batch   570  of  3,504.    Elapsed: 0:06:37.\n",
      "  Batch   580  of  3,504.    Elapsed: 0:06:44.\n",
      "  Batch   590  of  3,504.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  3,504.    Elapsed: 0:06:58.\n",
      "  Batch   610  of  3,504.    Elapsed: 0:07:05.\n",
      "  Batch   620  of  3,504.    Elapsed: 0:07:12.\n",
      "  Batch   630  of  3,504.    Elapsed: 0:07:19.\n",
      "  Batch   640  of  3,504.    Elapsed: 0:07:26.\n",
      "  Batch   650  of  3,504.    Elapsed: 0:07:33.\n",
      "  Batch   660  of  3,504.    Elapsed: 0:07:40.\n",
      "  Batch   670  of  3,504.    Elapsed: 0:07:46.\n",
      "  Batch   680  of  3,504.    Elapsed: 0:07:53.\n",
      "  Batch   690  of  3,504.    Elapsed: 0:08:00.\n",
      "  Batch   700  of  3,504.    Elapsed: 0:08:07.\n",
      "  Batch   710  of  3,504.    Elapsed: 0:08:14.\n",
      "  Batch   720  of  3,504.    Elapsed: 0:08:21.\n",
      "  Batch   730  of  3,504.    Elapsed: 0:08:28.\n",
      "  Batch   740  of  3,504.    Elapsed: 0:08:35.\n",
      "  Batch   750  of  3,504.    Elapsed: 0:08:42.\n",
      "  Batch   760  of  3,504.    Elapsed: 0:08:49.\n",
      "  Batch   770  of  3,504.    Elapsed: 0:08:56.\n",
      "  Batch   780  of  3,504.    Elapsed: 0:09:03.\n",
      "  Batch   790  of  3,504.    Elapsed: 0:09:10.\n",
      "  Batch   800  of  3,504.    Elapsed: 0:09:17.\n",
      "  Batch   810  of  3,504.    Elapsed: 0:09:24.\n",
      "  Batch   820  of  3,504.    Elapsed: 0:09:31.\n",
      "  Batch   830  of  3,504.    Elapsed: 0:09:38.\n",
      "  Batch   840  of  3,504.    Elapsed: 0:09:45.\n",
      "  Batch   850  of  3,504.    Elapsed: 0:09:52.\n",
      "  Batch   860  of  3,504.    Elapsed: 0:09:59.\n",
      "  Batch   870  of  3,504.    Elapsed: 0:10:06.\n",
      "  Batch   880  of  3,504.    Elapsed: 0:10:13.\n",
      "  Batch   890  of  3,504.    Elapsed: 0:10:20.\n",
      "  Batch   900  of  3,504.    Elapsed: 0:10:27.\n",
      "  Batch   910  of  3,504.    Elapsed: 0:10:34.\n",
      "  Batch   920  of  3,504.    Elapsed: 0:10:41.\n",
      "  Batch   930  of  3,504.    Elapsed: 0:10:48.\n",
      "  Batch   940  of  3,504.    Elapsed: 0:10:54.\n",
      "  Batch   950  of  3,504.    Elapsed: 0:11:01.\n",
      "  Batch   960  of  3,504.    Elapsed: 0:11:08.\n",
      "  Batch   970  of  3,504.    Elapsed: 0:11:15.\n",
      "  Batch   980  of  3,504.    Elapsed: 0:11:22.\n",
      "  Batch   990  of  3,504.    Elapsed: 0:11:29.\n",
      "  Batch 1,000  of  3,504.    Elapsed: 0:11:36.\n",
      "  Batch 1,010  of  3,504.    Elapsed: 0:11:43.\n",
      "  Batch 1,020  of  3,504.    Elapsed: 0:11:50.\n",
      "  Batch 1,030  of  3,504.    Elapsed: 0:11:57.\n",
      "  Batch 1,040  of  3,504.    Elapsed: 0:12:04.\n",
      "  Batch 1,050  of  3,504.    Elapsed: 0:12:11.\n",
      "  Batch 1,060  of  3,504.    Elapsed: 0:12:18.\n",
      "  Batch 1,070  of  3,504.    Elapsed: 0:12:25.\n",
      "  Batch 1,080  of  3,504.    Elapsed: 0:12:32.\n",
      "  Batch 1,090  of  3,504.    Elapsed: 0:12:39.\n",
      "  Batch 1,100  of  3,504.    Elapsed: 0:12:46.\n",
      "  Batch 1,110  of  3,504.    Elapsed: 0:12:53.\n",
      "  Batch 1,120  of  3,504.    Elapsed: 0:13:00.\n",
      "  Batch 1,130  of  3,504.    Elapsed: 0:13:07.\n",
      "  Batch 1,140  of  3,504.    Elapsed: 0:13:14.\n",
      "  Batch 1,150  of  3,504.    Elapsed: 0:13:21.\n",
      "  Batch 1,160  of  3,504.    Elapsed: 0:13:28.\n",
      "  Batch 1,170  of  3,504.    Elapsed: 0:13:35.\n",
      "  Batch 1,180  of  3,504.    Elapsed: 0:13:42.\n",
      "  Batch 1,190  of  3,504.    Elapsed: 0:13:49.\n",
      "  Batch 1,200  of  3,504.    Elapsed: 0:13:56.\n",
      "  Batch 1,210  of  3,504.    Elapsed: 0:14:02.\n",
      "  Batch 1,220  of  3,504.    Elapsed: 0:14:09.\n",
      "  Batch 1,230  of  3,504.    Elapsed: 0:14:16.\n",
      "  Batch 1,240  of  3,504.    Elapsed: 0:14:23.\n",
      "  Batch 1,250  of  3,504.    Elapsed: 0:14:30.\n",
      "  Batch 1,260  of  3,504.    Elapsed: 0:14:37.\n",
      "  Batch 1,270  of  3,504.    Elapsed: 0:14:44.\n",
      "  Batch 1,280  of  3,504.    Elapsed: 0:14:51.\n",
      "  Batch 1,290  of  3,504.    Elapsed: 0:14:58.\n",
      "  Batch 1,300  of  3,504.    Elapsed: 0:15:05.\n",
      "  Batch 1,310  of  3,504.    Elapsed: 0:15:12.\n",
      "  Batch 1,320  of  3,504.    Elapsed: 0:15:19.\n",
      "  Batch 1,330  of  3,504.    Elapsed: 0:15:26.\n",
      "  Batch 1,340  of  3,504.    Elapsed: 0:15:33.\n",
      "  Batch 1,350  of  3,504.    Elapsed: 0:15:40.\n",
      "  Batch 1,360  of  3,504.    Elapsed: 0:15:47.\n",
      "  Batch 1,370  of  3,504.    Elapsed: 0:15:54.\n",
      "  Batch 1,380  of  3,504.    Elapsed: 0:16:01.\n",
      "  Batch 1,390  of  3,504.    Elapsed: 0:16:08.\n",
      "  Batch 1,400  of  3,504.    Elapsed: 0:16:15.\n",
      "  Batch 1,410  of  3,504.    Elapsed: 0:16:22.\n",
      "  Batch 1,420  of  3,504.    Elapsed: 0:16:29.\n",
      "  Batch 1,430  of  3,504.    Elapsed: 0:16:36.\n",
      "  Batch 1,440  of  3,504.    Elapsed: 0:16:43.\n",
      "  Batch 1,450  of  3,504.    Elapsed: 0:16:50.\n",
      "  Batch 1,460  of  3,504.    Elapsed: 0:16:57.\n",
      "  Batch 1,470  of  3,504.    Elapsed: 0:17:03.\n",
      "  Batch 1,480  of  3,504.    Elapsed: 0:17:10.\n",
      "  Batch 1,490  of  3,504.    Elapsed: 0:17:17.\n",
      "  Batch 1,500  of  3,504.    Elapsed: 0:17:24.\n",
      "  Batch 1,510  of  3,504.    Elapsed: 0:17:31.\n",
      "  Batch 1,520  of  3,504.    Elapsed: 0:17:38.\n",
      "  Batch 1,530  of  3,504.    Elapsed: 0:17:45.\n",
      "  Batch 1,540  of  3,504.    Elapsed: 0:17:52.\n",
      "  Batch 1,550  of  3,504.    Elapsed: 0:17:59.\n",
      "  Batch 1,560  of  3,504.    Elapsed: 0:18:06.\n",
      "  Batch 1,570  of  3,504.    Elapsed: 0:18:13.\n",
      "  Batch 1,580  of  3,504.    Elapsed: 0:18:20.\n",
      "  Batch 1,590  of  3,504.    Elapsed: 0:18:27.\n",
      "  Batch 1,600  of  3,504.    Elapsed: 0:18:34.\n",
      "  Batch 1,610  of  3,504.    Elapsed: 0:18:41.\n",
      "  Batch 1,620  of  3,504.    Elapsed: 0:18:48.\n",
      "  Batch 1,630  of  3,504.    Elapsed: 0:18:55.\n",
      "  Batch 1,640  of  3,504.    Elapsed: 0:19:02.\n",
      "  Batch 1,650  of  3,504.    Elapsed: 0:19:09.\n",
      "  Batch 1,660  of  3,504.    Elapsed: 0:19:16.\n",
      "  Batch 1,670  of  3,504.    Elapsed: 0:19:23.\n",
      "  Batch 1,680  of  3,504.    Elapsed: 0:19:30.\n",
      "  Batch 1,690  of  3,504.    Elapsed: 0:19:37.\n",
      "  Batch 1,700  of  3,504.    Elapsed: 0:19:44.\n",
      "  Batch 1,710  of  3,504.    Elapsed: 0:19:51.\n",
      "  Batch 1,720  of  3,504.    Elapsed: 0:19:58.\n",
      "  Batch 1,730  of  3,504.    Elapsed: 0:20:05.\n",
      "  Batch 1,740  of  3,504.    Elapsed: 0:20:12.\n",
      "  Batch 1,750  of  3,504.    Elapsed: 0:20:19.\n",
      "  Batch 1,760  of  3,504.    Elapsed: 0:20:26.\n",
      "  Batch 1,770  of  3,504.    Elapsed: 0:20:33.\n",
      "  Batch 1,780  of  3,504.    Elapsed: 0:20:40.\n",
      "  Batch 1,790  of  3,504.    Elapsed: 0:20:47.\n",
      "  Batch 1,800  of  3,504.    Elapsed: 0:20:54.\n",
      "  Batch 1,810  of  3,504.    Elapsed: 0:21:01.\n",
      "  Batch 1,820  of  3,504.    Elapsed: 0:21:08.\n",
      "  Batch 1,830  of  3,504.    Elapsed: 0:21:14.\n",
      "  Batch 1,840  of  3,504.    Elapsed: 0:21:21.\n",
      "  Batch 1,850  of  3,504.    Elapsed: 0:21:28.\n",
      "  Batch 1,860  of  3,504.    Elapsed: 0:21:35.\n",
      "  Batch 1,870  of  3,504.    Elapsed: 0:21:42.\n",
      "  Batch 1,880  of  3,504.    Elapsed: 0:21:49.\n",
      "  Batch 1,890  of  3,504.    Elapsed: 0:21:56.\n",
      "  Batch 1,900  of  3,504.    Elapsed: 0:22:03.\n",
      "  Batch 1,910  of  3,504.    Elapsed: 0:22:10.\n",
      "  Batch 1,920  of  3,504.    Elapsed: 0:22:17.\n",
      "  Batch 1,930  of  3,504.    Elapsed: 0:22:24.\n",
      "  Batch 1,940  of  3,504.    Elapsed: 0:22:31.\n",
      "  Batch 1,950  of  3,504.    Elapsed: 0:22:38.\n",
      "  Batch 1,960  of  3,504.    Elapsed: 0:22:45.\n",
      "  Batch 1,970  of  3,504.    Elapsed: 0:22:52.\n",
      "  Batch 1,980  of  3,504.    Elapsed: 0:22:59.\n",
      "  Batch 1,990  of  3,504.    Elapsed: 0:23:06.\n",
      "  Batch 2,000  of  3,504.    Elapsed: 0:23:13.\n",
      "  Batch 2,010  of  3,504.    Elapsed: 0:23:20.\n",
      "  Batch 2,020  of  3,504.    Elapsed: 0:23:27.\n",
      "  Batch 2,030  of  3,504.    Elapsed: 0:23:34.\n",
      "  Batch 2,040  of  3,504.    Elapsed: 0:23:41.\n",
      "  Batch 2,050  of  3,504.    Elapsed: 0:23:48.\n",
      "  Batch 2,060  of  3,504.    Elapsed: 0:23:55.\n",
      "  Batch 2,070  of  3,504.    Elapsed: 0:24:02.\n",
      "  Batch 2,080  of  3,504.    Elapsed: 0:24:09.\n",
      "  Batch 2,090  of  3,504.    Elapsed: 0:24:16.\n",
      "  Batch 2,100  of  3,504.    Elapsed: 0:24:23.\n",
      "  Batch 2,110  of  3,504.    Elapsed: 0:24:30.\n",
      "  Batch 2,120  of  3,504.    Elapsed: 0:24:36.\n",
      "  Batch 2,130  of  3,504.    Elapsed: 0:24:43.\n",
      "  Batch 2,140  of  3,504.    Elapsed: 0:24:50.\n",
      "  Batch 2,150  of  3,504.    Elapsed: 0:24:57.\n",
      "  Batch 2,160  of  3,504.    Elapsed: 0:25:04.\n",
      "  Batch 2,170  of  3,504.    Elapsed: 0:25:11.\n",
      "  Batch 2,180  of  3,504.    Elapsed: 0:25:18.\n",
      "  Batch 2,190  of  3,504.    Elapsed: 0:25:25.\n",
      "  Batch 2,200  of  3,504.    Elapsed: 0:25:32.\n",
      "  Batch 2,210  of  3,504.    Elapsed: 0:25:39.\n",
      "  Batch 2,220  of  3,504.    Elapsed: 0:25:46.\n",
      "  Batch 2,230  of  3,504.    Elapsed: 0:25:53.\n",
      "  Batch 2,240  of  3,504.    Elapsed: 0:26:00.\n",
      "  Batch 2,250  of  3,504.    Elapsed: 0:26:07.\n",
      "  Batch 2,260  of  3,504.    Elapsed: 0:26:14.\n",
      "  Batch 2,270  of  3,504.    Elapsed: 0:26:21.\n",
      "  Batch 2,280  of  3,504.    Elapsed: 0:26:28.\n",
      "  Batch 2,290  of  3,504.    Elapsed: 0:26:35.\n",
      "  Batch 2,300  of  3,504.    Elapsed: 0:26:42.\n",
      "  Batch 2,310  of  3,504.    Elapsed: 0:26:49.\n",
      "  Batch 2,320  of  3,504.    Elapsed: 0:26:56.\n",
      "  Batch 2,330  of  3,504.    Elapsed: 0:27:03.\n",
      "  Batch 2,340  of  3,504.    Elapsed: 0:27:10.\n",
      "  Batch 2,350  of  3,504.    Elapsed: 0:27:17.\n",
      "  Batch 2,360  of  3,504.    Elapsed: 0:27:24.\n",
      "  Batch 2,370  of  3,504.    Elapsed: 0:27:31.\n",
      "  Batch 2,380  of  3,504.    Elapsed: 0:27:38.\n",
      "  Batch 2,390  of  3,504.    Elapsed: 0:27:45.\n",
      "  Batch 2,400  of  3,504.    Elapsed: 0:27:52.\n",
      "  Batch 2,410  of  3,504.    Elapsed: 0:27:59.\n",
      "  Batch 2,420  of  3,504.    Elapsed: 0:28:06.\n",
      "  Batch 2,430  of  3,504.    Elapsed: 0:28:13.\n",
      "  Batch 2,440  of  3,504.    Elapsed: 0:28:20.\n",
      "  Batch 2,450  of  3,504.    Elapsed: 0:28:26.\n",
      "  Batch 2,460  of  3,504.    Elapsed: 0:28:33.\n",
      "  Batch 2,470  of  3,504.    Elapsed: 0:28:40.\n",
      "  Batch 2,480  of  3,504.    Elapsed: 0:28:47.\n",
      "  Batch 2,490  of  3,504.    Elapsed: 0:28:54.\n",
      "  Batch 2,500  of  3,504.    Elapsed: 0:29:01.\n",
      "  Batch 2,510  of  3,504.    Elapsed: 0:29:08.\n",
      "  Batch 2,520  of  3,504.    Elapsed: 0:29:15.\n",
      "  Batch 2,530  of  3,504.    Elapsed: 0:29:22.\n",
      "  Batch 2,540  of  3,504.    Elapsed: 0:29:29.\n",
      "  Batch 2,550  of  3,504.    Elapsed: 0:29:36.\n",
      "  Batch 2,560  of  3,504.    Elapsed: 0:29:43.\n",
      "  Batch 2,570  of  3,504.    Elapsed: 0:29:50.\n",
      "  Batch 2,580  of  3,504.    Elapsed: 0:29:57.\n",
      "  Batch 2,590  of  3,504.    Elapsed: 0:30:04.\n",
      "  Batch 2,600  of  3,504.    Elapsed: 0:30:11.\n",
      "  Batch 2,610  of  3,504.    Elapsed: 0:30:18.\n",
      "  Batch 2,620  of  3,504.    Elapsed: 0:30:25.\n",
      "  Batch 2,630  of  3,504.    Elapsed: 0:30:32.\n",
      "  Batch 2,640  of  3,504.    Elapsed: 0:30:39.\n",
      "  Batch 2,650  of  3,504.    Elapsed: 0:30:46.\n",
      "  Batch 2,660  of  3,504.    Elapsed: 0:30:53.\n",
      "  Batch 2,670  of  3,504.    Elapsed: 0:31:00.\n",
      "  Batch 2,680  of  3,504.    Elapsed: 0:31:07.\n",
      "  Batch 2,690  of  3,504.    Elapsed: 0:31:14.\n",
      "  Batch 2,700  of  3,504.    Elapsed: 0:31:21.\n",
      "  Batch 2,710  of  3,504.    Elapsed: 0:31:28.\n",
      "  Batch 2,720  of  3,504.    Elapsed: 0:31:34.\n",
      "  Batch 2,730  of  3,504.    Elapsed: 0:31:41.\n",
      "  Batch 2,740  of  3,504.    Elapsed: 0:31:48.\n",
      "  Batch 2,750  of  3,504.    Elapsed: 0:31:55.\n",
      "  Batch 2,760  of  3,504.    Elapsed: 0:32:02.\n",
      "  Batch 2,770  of  3,504.    Elapsed: 0:32:09.\n",
      "  Batch 2,780  of  3,504.    Elapsed: 0:32:16.\n",
      "  Batch 2,790  of  3,504.    Elapsed: 0:32:23.\n",
      "  Batch 2,800  of  3,504.    Elapsed: 0:32:30.\n",
      "  Batch 2,810  of  3,504.    Elapsed: 0:32:37.\n",
      "  Batch 2,820  of  3,504.    Elapsed: 0:32:44.\n",
      "  Batch 2,830  of  3,504.    Elapsed: 0:32:51.\n",
      "  Batch 2,840  of  3,504.    Elapsed: 0:32:58.\n",
      "  Batch 2,850  of  3,504.    Elapsed: 0:33:05.\n",
      "  Batch 2,860  of  3,504.    Elapsed: 0:33:12.\n",
      "  Batch 2,870  of  3,504.    Elapsed: 0:33:19.\n",
      "  Batch 2,880  of  3,504.    Elapsed: 0:33:26.\n",
      "  Batch 2,890  of  3,504.    Elapsed: 0:33:33.\n",
      "  Batch 2,900  of  3,504.    Elapsed: 0:33:40.\n",
      "  Batch 2,910  of  3,504.    Elapsed: 0:33:47.\n",
      "  Batch 2,920  of  3,504.    Elapsed: 0:33:54.\n",
      "  Batch 2,930  of  3,504.    Elapsed: 0:34:01.\n",
      "  Batch 2,940  of  3,504.    Elapsed: 0:34:08.\n",
      "  Batch 2,950  of  3,504.    Elapsed: 0:34:14.\n",
      "  Batch 2,960  of  3,504.    Elapsed: 0:34:21.\n",
      "  Batch 2,970  of  3,504.    Elapsed: 0:34:28.\n",
      "  Batch 2,980  of  3,504.    Elapsed: 0:34:35.\n",
      "  Batch 2,990  of  3,504.    Elapsed: 0:34:42.\n",
      "  Batch 3,000  of  3,504.    Elapsed: 0:34:49.\n",
      "  Batch 3,010  of  3,504.    Elapsed: 0:34:56.\n",
      "  Batch 3,020  of  3,504.    Elapsed: 0:35:03.\n",
      "  Batch 3,030  of  3,504.    Elapsed: 0:35:10.\n",
      "  Batch 3,040  of  3,504.    Elapsed: 0:35:17.\n",
      "  Batch 3,050  of  3,504.    Elapsed: 0:35:24.\n",
      "  Batch 3,060  of  3,504.    Elapsed: 0:35:31.\n",
      "  Batch 3,070  of  3,504.    Elapsed: 0:35:38.\n",
      "  Batch 3,080  of  3,504.    Elapsed: 0:35:45.\n",
      "  Batch 3,090  of  3,504.    Elapsed: 0:35:52.\n",
      "  Batch 3,100  of  3,504.    Elapsed: 0:35:59.\n",
      "  Batch 3,110  of  3,504.    Elapsed: 0:36:06.\n",
      "  Batch 3,120  of  3,504.    Elapsed: 0:36:13.\n",
      "  Batch 3,130  of  3,504.    Elapsed: 0:36:20.\n",
      "  Batch 3,140  of  3,504.    Elapsed: 0:36:27.\n",
      "  Batch 3,150  of  3,504.    Elapsed: 0:36:34.\n",
      "  Batch 3,160  of  3,504.    Elapsed: 0:36:41.\n",
      "  Batch 3,170  of  3,504.    Elapsed: 0:36:48.\n",
      "  Batch 3,180  of  3,504.    Elapsed: 0:36:55.\n",
      "  Batch 3,190  of  3,504.    Elapsed: 0:37:02.\n",
      "  Batch 3,200  of  3,504.    Elapsed: 0:37:09.\n",
      "  Batch 3,210  of  3,504.    Elapsed: 0:37:16.\n",
      "  Batch 3,220  of  3,504.    Elapsed: 0:37:23.\n",
      "  Batch 3,230  of  3,504.    Elapsed: 0:37:30.\n",
      "  Batch 3,240  of  3,504.    Elapsed: 0:37:36.\n",
      "  Batch 3,250  of  3,504.    Elapsed: 0:37:43.\n",
      "  Batch 3,260  of  3,504.    Elapsed: 0:37:50.\n",
      "  Batch 3,270  of  3,504.    Elapsed: 0:37:57.\n",
      "  Batch 3,280  of  3,504.    Elapsed: 0:38:04.\n",
      "  Batch 3,290  of  3,504.    Elapsed: 0:38:11.\n",
      "  Batch 3,300  of  3,504.    Elapsed: 0:38:18.\n",
      "  Batch 3,310  of  3,504.    Elapsed: 0:38:25.\n",
      "  Batch 3,320  of  3,504.    Elapsed: 0:38:32.\n",
      "  Batch 3,330  of  3,504.    Elapsed: 0:38:39.\n",
      "  Batch 3,340  of  3,504.    Elapsed: 0:38:46.\n",
      "  Batch 3,350  of  3,504.    Elapsed: 0:38:53.\n",
      "  Batch 3,360  of  3,504.    Elapsed: 0:39:00.\n",
      "  Batch 3,370  of  3,504.    Elapsed: 0:39:07.\n",
      "  Batch 3,380  of  3,504.    Elapsed: 0:39:14.\n",
      "  Batch 3,390  of  3,504.    Elapsed: 0:39:21.\n",
      "  Batch 3,400  of  3,504.    Elapsed: 0:39:28.\n",
      "  Batch 3,410  of  3,504.    Elapsed: 0:39:35.\n",
      "  Batch 3,420  of  3,504.    Elapsed: 0:39:42.\n",
      "  Batch 3,430  of  3,504.    Elapsed: 0:39:49.\n",
      "  Batch 3,440  of  3,504.    Elapsed: 0:39:56.\n",
      "  Batch 3,450  of  3,504.    Elapsed: 0:40:03.\n",
      "  Batch 3,460  of  3,504.    Elapsed: 0:40:10.\n",
      "  Batch 3,470  of  3,504.    Elapsed: 0:40:17.\n",
      "  Batch 3,480  of  3,504.    Elapsed: 0:40:24.\n",
      "  Batch 3,490  of  3,504.    Elapsed: 0:40:31.\n",
      "  Batch 3,500  of  3,504.    Elapsed: 0:40:37.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 1.065\n",
      "  Training epcoh took: 0:40:40\n",
      "\n",
      "Running Test...\n",
      "all_preds\n",
      "[2 1 1 ... 2 4 5]\n",
      "all_labels\n",
      "[3 1 1 ... 3 5 4]\n",
      "  Accuracy: 0.536\n",
      "  Test Loss: 1.775\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of  3,504.    Elapsed: 0:00:07.\n",
      "  Batch    20  of  3,504.    Elapsed: 0:00:14.\n",
      "  Batch    30  of  3,504.    Elapsed: 0:00:21.\n",
      "  Batch    40  of  3,504.    Elapsed: 0:00:28.\n",
      "  Batch    50  of  3,504.    Elapsed: 0:00:35.\n",
      "  Batch    60  of  3,504.    Elapsed: 0:00:42.\n",
      "  Batch    70  of  3,504.    Elapsed: 0:00:49.\n",
      "  Batch    80  of  3,504.    Elapsed: 0:00:56.\n",
      "  Batch    90  of  3,504.    Elapsed: 0:01:03.\n",
      "  Batch   100  of  3,504.    Elapsed: 0:01:10.\n",
      "  Batch   110  of  3,504.    Elapsed: 0:01:17.\n",
      "  Batch   120  of  3,504.    Elapsed: 0:01:24.\n",
      "  Batch   130  of  3,504.    Elapsed: 0:01:31.\n",
      "  Batch   140  of  3,504.    Elapsed: 0:01:37.\n",
      "  Batch   150  of  3,504.    Elapsed: 0:01:44.\n",
      "  Batch   160  of  3,504.    Elapsed: 0:01:51.\n",
      "  Batch   170  of  3,504.    Elapsed: 0:01:58.\n",
      "  Batch   180  of  3,504.    Elapsed: 0:02:05.\n",
      "  Batch   190  of  3,504.    Elapsed: 0:02:12.\n",
      "  Batch   200  of  3,504.    Elapsed: 0:02:19.\n",
      "  Batch   210  of  3,504.    Elapsed: 0:02:26.\n",
      "  Batch   220  of  3,504.    Elapsed: 0:02:33.\n",
      "  Batch   230  of  3,504.    Elapsed: 0:02:40.\n",
      "  Batch   240  of  3,504.    Elapsed: 0:02:47.\n",
      "  Batch   250  of  3,504.    Elapsed: 0:02:54.\n",
      "  Batch   260  of  3,504.    Elapsed: 0:03:01.\n",
      "  Batch   270  of  3,504.    Elapsed: 0:03:08.\n",
      "  Batch   280  of  3,504.    Elapsed: 0:03:15.\n",
      "  Batch   290  of  3,504.    Elapsed: 0:03:22.\n",
      "  Batch   300  of  3,504.    Elapsed: 0:03:29.\n",
      "  Batch   310  of  3,504.    Elapsed: 0:03:36.\n",
      "  Batch   320  of  3,504.    Elapsed: 0:03:43.\n",
      "  Batch   330  of  3,504.    Elapsed: 0:03:50.\n",
      "  Batch   340  of  3,504.    Elapsed: 0:03:57.\n",
      "  Batch   350  of  3,504.    Elapsed: 0:04:04.\n",
      "  Batch   360  of  3,504.    Elapsed: 0:04:11.\n",
      "  Batch   370  of  3,504.    Elapsed: 0:04:18.\n",
      "  Batch   380  of  3,504.    Elapsed: 0:04:25.\n",
      "  Batch   390  of  3,504.    Elapsed: 0:04:32.\n",
      "  Batch   400  of  3,504.    Elapsed: 0:04:39.\n",
      "  Batch   410  of  3,504.    Elapsed: 0:04:45.\n",
      "  Batch   420  of  3,504.    Elapsed: 0:04:52.\n",
      "  Batch   430  of  3,504.    Elapsed: 0:04:59.\n",
      "  Batch   440  of  3,504.    Elapsed: 0:05:06.\n",
      "  Batch   450  of  3,504.    Elapsed: 0:05:13.\n",
      "  Batch   460  of  3,504.    Elapsed: 0:05:20.\n",
      "  Batch   470  of  3,504.    Elapsed: 0:05:27.\n",
      "  Batch   480  of  3,504.    Elapsed: 0:05:34.\n",
      "  Batch   490  of  3,504.    Elapsed: 0:05:41.\n",
      "  Batch   500  of  3,504.    Elapsed: 0:05:48.\n",
      "  Batch   510  of  3,504.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  3,504.    Elapsed: 0:06:02.\n",
      "  Batch   530  of  3,504.    Elapsed: 0:06:09.\n",
      "  Batch   540  of  3,504.    Elapsed: 0:06:16.\n",
      "  Batch   550  of  3,504.    Elapsed: 0:06:23.\n",
      "  Batch   560  of  3,504.    Elapsed: 0:06:30.\n",
      "  Batch   570  of  3,504.    Elapsed: 0:06:37.\n",
      "  Batch   580  of  3,504.    Elapsed: 0:06:44.\n",
      "  Batch   590  of  3,504.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  3,504.    Elapsed: 0:06:58.\n",
      "  Batch   610  of  3,504.    Elapsed: 0:07:05.\n",
      "  Batch   620  of  3,504.    Elapsed: 0:07:12.\n",
      "  Batch   630  of  3,504.    Elapsed: 0:07:19.\n",
      "  Batch   640  of  3,504.    Elapsed: 0:07:26.\n",
      "  Batch   650  of  3,504.    Elapsed: 0:07:33.\n",
      "  Batch   660  of  3,504.    Elapsed: 0:07:40.\n",
      "  Batch   670  of  3,504.    Elapsed: 0:07:47.\n",
      "  Batch   680  of  3,504.    Elapsed: 0:07:53.\n",
      "  Batch   690  of  3,504.    Elapsed: 0:08:00.\n",
      "  Batch   700  of  3,504.    Elapsed: 0:08:07.\n",
      "  Batch   710  of  3,504.    Elapsed: 0:08:14.\n",
      "  Batch   720  of  3,504.    Elapsed: 0:08:21.\n",
      "  Batch   730  of  3,504.    Elapsed: 0:08:28.\n",
      "  Batch   740  of  3,504.    Elapsed: 0:08:35.\n",
      "  Batch   750  of  3,504.    Elapsed: 0:08:42.\n",
      "  Batch   760  of  3,504.    Elapsed: 0:08:49.\n",
      "  Batch   770  of  3,504.    Elapsed: 0:08:56.\n",
      "  Batch   780  of  3,504.    Elapsed: 0:09:03.\n",
      "  Batch   790  of  3,504.    Elapsed: 0:09:10.\n",
      "  Batch   800  of  3,504.    Elapsed: 0:09:17.\n",
      "  Batch   810  of  3,504.    Elapsed: 0:09:24.\n",
      "  Batch   820  of  3,504.    Elapsed: 0:09:31.\n",
      "  Batch   830  of  3,504.    Elapsed: 0:09:38.\n",
      "  Batch   840  of  3,504.    Elapsed: 0:09:45.\n",
      "  Batch   850  of  3,504.    Elapsed: 0:09:52.\n",
      "  Batch   860  of  3,504.    Elapsed: 0:09:59.\n",
      "  Batch   870  of  3,504.    Elapsed: 0:10:06.\n",
      "  Batch   880  of  3,504.    Elapsed: 0:10:13.\n",
      "  Batch   890  of  3,504.    Elapsed: 0:10:20.\n",
      "  Batch   900  of  3,504.    Elapsed: 0:10:27.\n",
      "  Batch   910  of  3,504.    Elapsed: 0:10:34.\n",
      "  Batch   920  of  3,504.    Elapsed: 0:10:41.\n",
      "  Batch   930  of  3,504.    Elapsed: 0:10:48.\n",
      "  Batch   940  of  3,504.    Elapsed: 0:10:54.\n",
      "  Batch   950  of  3,504.    Elapsed: 0:11:01.\n",
      "  Batch   960  of  3,504.    Elapsed: 0:11:08.\n",
      "  Batch   970  of  3,504.    Elapsed: 0:11:15.\n",
      "  Batch   980  of  3,504.    Elapsed: 0:11:22.\n",
      "  Batch   990  of  3,504.    Elapsed: 0:11:29.\n",
      "  Batch 1,000  of  3,504.    Elapsed: 0:11:36.\n",
      "  Batch 1,010  of  3,504.    Elapsed: 0:11:43.\n",
      "  Batch 1,020  of  3,504.    Elapsed: 0:11:50.\n",
      "  Batch 1,030  of  3,504.    Elapsed: 0:11:57.\n",
      "  Batch 1,040  of  3,504.    Elapsed: 0:12:04.\n",
      "  Batch 1,050  of  3,504.    Elapsed: 0:12:11.\n",
      "  Batch 1,060  of  3,504.    Elapsed: 0:12:18.\n",
      "  Batch 1,070  of  3,504.    Elapsed: 0:12:25.\n",
      "  Batch 1,080  of  3,504.    Elapsed: 0:12:32.\n",
      "  Batch 1,090  of  3,504.    Elapsed: 0:12:39.\n",
      "  Batch 1,100  of  3,504.    Elapsed: 0:12:46.\n",
      "  Batch 1,110  of  3,504.    Elapsed: 0:12:53.\n",
      "  Batch 1,120  of  3,504.    Elapsed: 0:13:00.\n",
      "  Batch 1,130  of  3,504.    Elapsed: 0:13:07.\n",
      "  Batch 1,140  of  3,504.    Elapsed: 0:13:14.\n",
      "  Batch 1,150  of  3,504.    Elapsed: 0:13:21.\n",
      "  Batch 1,160  of  3,504.    Elapsed: 0:13:28.\n",
      "  Batch 1,170  of  3,504.    Elapsed: 0:13:35.\n",
      "  Batch 1,180  of  3,504.    Elapsed: 0:13:42.\n",
      "  Batch 1,190  of  3,504.    Elapsed: 0:13:49.\n",
      "  Batch 1,200  of  3,504.    Elapsed: 0:13:56.\n",
      "  Batch 1,210  of  3,504.    Elapsed: 0:14:03.\n",
      "  Batch 1,220  of  3,504.    Elapsed: 0:14:10.\n",
      "  Batch 1,230  of  3,504.    Elapsed: 0:14:17.\n",
      "  Batch 1,240  of  3,504.    Elapsed: 0:14:23.\n",
      "  Batch 1,250  of  3,504.    Elapsed: 0:14:30.\n",
      "  Batch 1,260  of  3,504.    Elapsed: 0:14:37.\n",
      "  Batch 1,270  of  3,504.    Elapsed: 0:14:44.\n",
      "  Batch 1,280  of  3,504.    Elapsed: 0:14:51.\n",
      "  Batch 1,290  of  3,504.    Elapsed: 0:14:58.\n",
      "  Batch 1,300  of  3,504.    Elapsed: 0:15:05.\n",
      "  Batch 1,310  of  3,504.    Elapsed: 0:15:12.\n",
      "  Batch 1,320  of  3,504.    Elapsed: 0:15:19.\n",
      "  Batch 1,330  of  3,504.    Elapsed: 0:15:26.\n",
      "  Batch 1,340  of  3,504.    Elapsed: 0:15:33.\n",
      "  Batch 1,350  of  3,504.    Elapsed: 0:15:40.\n",
      "  Batch 1,360  of  3,504.    Elapsed: 0:15:47.\n",
      "  Batch 1,370  of  3,504.    Elapsed: 0:15:54.\n",
      "  Batch 1,380  of  3,504.    Elapsed: 0:16:01.\n",
      "  Batch 1,390  of  3,504.    Elapsed: 0:16:08.\n",
      "  Batch 1,400  of  3,504.    Elapsed: 0:16:15.\n",
      "  Batch 1,410  of  3,504.    Elapsed: 0:16:22.\n",
      "  Batch 1,420  of  3,504.    Elapsed: 0:16:29.\n",
      "  Batch 1,430  of  3,504.    Elapsed: 0:16:36.\n",
      "  Batch 1,440  of  3,504.    Elapsed: 0:16:43.\n",
      "  Batch 1,450  of  3,504.    Elapsed: 0:16:50.\n",
      "  Batch 1,460  of  3,504.    Elapsed: 0:16:57.\n",
      "  Batch 1,470  of  3,504.    Elapsed: 0:17:04.\n",
      "  Batch 1,480  of  3,504.    Elapsed: 0:17:10.\n",
      "  Batch 1,490  of  3,504.    Elapsed: 0:17:17.\n",
      "  Batch 1,500  of  3,504.    Elapsed: 0:17:24.\n",
      "  Batch 1,510  of  3,504.    Elapsed: 0:17:31.\n",
      "  Batch 1,520  of  3,504.    Elapsed: 0:17:38.\n",
      "  Batch 1,530  of  3,504.    Elapsed: 0:17:45.\n",
      "  Batch 1,540  of  3,504.    Elapsed: 0:17:52.\n",
      "  Batch 1,550  of  3,504.    Elapsed: 0:17:59.\n",
      "  Batch 1,560  of  3,504.    Elapsed: 0:18:06.\n",
      "  Batch 1,570  of  3,504.    Elapsed: 0:18:13.\n",
      "  Batch 1,580  of  3,504.    Elapsed: 0:18:20.\n",
      "  Batch 1,590  of  3,504.    Elapsed: 0:18:27.\n",
      "  Batch 1,600  of  3,504.    Elapsed: 0:18:34.\n",
      "  Batch 1,610  of  3,504.    Elapsed: 0:18:41.\n",
      "  Batch 1,620  of  3,504.    Elapsed: 0:18:48.\n",
      "  Batch 1,630  of  3,504.    Elapsed: 0:18:55.\n",
      "  Batch 1,640  of  3,504.    Elapsed: 0:19:02.\n",
      "  Batch 1,650  of  3,504.    Elapsed: 0:19:09.\n",
      "  Batch 1,660  of  3,504.    Elapsed: 0:19:16.\n",
      "  Batch 1,670  of  3,504.    Elapsed: 0:19:23.\n",
      "  Batch 1,680  of  3,504.    Elapsed: 0:19:30.\n",
      "  Batch 1,690  of  3,504.    Elapsed: 0:19:37.\n",
      "  Batch 1,700  of  3,504.    Elapsed: 0:19:44.\n",
      "  Batch 1,710  of  3,504.    Elapsed: 0:19:51.\n",
      "  Batch 1,720  of  3,504.    Elapsed: 0:19:58.\n",
      "  Batch 1,730  of  3,504.    Elapsed: 0:20:05.\n",
      "  Batch 1,740  of  3,504.    Elapsed: 0:20:11.\n",
      "  Batch 1,750  of  3,504.    Elapsed: 0:20:18.\n",
      "  Batch 1,760  of  3,504.    Elapsed: 0:20:25.\n",
      "  Batch 1,770  of  3,504.    Elapsed: 0:20:32.\n",
      "  Batch 1,780  of  3,504.    Elapsed: 0:20:39.\n",
      "  Batch 1,790  of  3,504.    Elapsed: 0:20:46.\n",
      "  Batch 1,800  of  3,504.    Elapsed: 0:20:53.\n",
      "  Batch 1,810  of  3,504.    Elapsed: 0:21:00.\n",
      "  Batch 1,820  of  3,504.    Elapsed: 0:21:07.\n",
      "  Batch 1,830  of  3,504.    Elapsed: 0:21:14.\n",
      "  Batch 1,840  of  3,504.    Elapsed: 0:21:21.\n",
      "  Batch 1,850  of  3,504.    Elapsed: 0:21:28.\n",
      "  Batch 1,860  of  3,504.    Elapsed: 0:21:35.\n",
      "  Batch 1,870  of  3,504.    Elapsed: 0:21:42.\n",
      "  Batch 1,880  of  3,504.    Elapsed: 0:21:49.\n",
      "  Batch 1,890  of  3,504.    Elapsed: 0:21:56.\n",
      "  Batch 1,900  of  3,504.    Elapsed: 0:22:03.\n",
      "  Batch 1,910  of  3,504.    Elapsed: 0:22:10.\n",
      "  Batch 1,920  of  3,504.    Elapsed: 0:22:17.\n",
      "  Batch 1,930  of  3,504.    Elapsed: 0:22:24.\n",
      "  Batch 1,940  of  3,504.    Elapsed: 0:22:31.\n",
      "  Batch 1,950  of  3,504.    Elapsed: 0:22:38.\n",
      "  Batch 1,960  of  3,504.    Elapsed: 0:22:45.\n",
      "  Batch 1,970  of  3,504.    Elapsed: 0:22:52.\n",
      "  Batch 1,980  of  3,504.    Elapsed: 0:22:59.\n",
      "  Batch 1,990  of  3,504.    Elapsed: 0:23:06.\n",
      "  Batch 2,000  of  3,504.    Elapsed: 0:23:12.\n",
      "  Batch 2,010  of  3,504.    Elapsed: 0:23:19.\n",
      "  Batch 2,020  of  3,504.    Elapsed: 0:23:26.\n",
      "  Batch 2,030  of  3,504.    Elapsed: 0:23:33.\n",
      "  Batch 2,040  of  3,504.    Elapsed: 0:23:40.\n",
      "  Batch 2,050  of  3,504.    Elapsed: 0:23:47.\n",
      "  Batch 2,060  of  3,504.    Elapsed: 0:23:54.\n",
      "  Batch 2,070  of  3,504.    Elapsed: 0:24:01.\n",
      "  Batch 2,080  of  3,504.    Elapsed: 0:24:08.\n",
      "  Batch 2,090  of  3,504.    Elapsed: 0:24:15.\n",
      "  Batch 2,100  of  3,504.    Elapsed: 0:24:22.\n",
      "  Batch 2,110  of  3,504.    Elapsed: 0:24:29.\n",
      "  Batch 2,120  of  3,504.    Elapsed: 0:24:36.\n",
      "  Batch 2,130  of  3,504.    Elapsed: 0:24:43.\n",
      "  Batch 2,140  of  3,504.    Elapsed: 0:24:50.\n",
      "  Batch 2,150  of  3,504.    Elapsed: 0:24:57.\n",
      "  Batch 2,160  of  3,504.    Elapsed: 0:25:04.\n",
      "  Batch 2,170  of  3,504.    Elapsed: 0:25:11.\n",
      "  Batch 2,180  of  3,504.    Elapsed: 0:25:18.\n",
      "  Batch 2,190  of  3,504.    Elapsed: 0:25:25.\n",
      "  Batch 2,200  of  3,504.    Elapsed: 0:25:32.\n",
      "  Batch 2,210  of  3,504.    Elapsed: 0:25:39.\n",
      "  Batch 2,220  of  3,504.    Elapsed: 0:25:46.\n",
      "  Batch 2,230  of  3,504.    Elapsed: 0:25:53.\n",
      "  Batch 2,240  of  3,504.    Elapsed: 0:26:00.\n",
      "  Batch 2,250  of  3,504.    Elapsed: 0:26:07.\n",
      "  Batch 2,260  of  3,504.    Elapsed: 0:26:14.\n",
      "  Batch 2,270  of  3,504.    Elapsed: 0:26:21.\n",
      "  Batch 2,280  of  3,504.    Elapsed: 0:26:28.\n",
      "  Batch 2,290  of  3,504.    Elapsed: 0:26:35.\n",
      "  Batch 2,300  of  3,504.    Elapsed: 0:26:42.\n",
      "  Batch 2,310  of  3,504.    Elapsed: 0:26:49.\n",
      "  Batch 2,320  of  3,504.    Elapsed: 0:26:55.\n",
      "  Batch 2,330  of  3,504.    Elapsed: 0:27:02.\n",
      "  Batch 2,340  of  3,504.    Elapsed: 0:27:09.\n",
      "  Batch 2,350  of  3,504.    Elapsed: 0:27:16.\n",
      "  Batch 2,360  of  3,504.    Elapsed: 0:27:23.\n",
      "  Batch 2,370  of  3,504.    Elapsed: 0:27:30.\n",
      "  Batch 2,380  of  3,504.    Elapsed: 0:27:37.\n",
      "  Batch 2,390  of  3,504.    Elapsed: 0:27:44.\n",
      "  Batch 2,400  of  3,504.    Elapsed: 0:27:51.\n",
      "  Batch 2,410  of  3,504.    Elapsed: 0:27:58.\n",
      "  Batch 2,420  of  3,504.    Elapsed: 0:28:05.\n",
      "  Batch 2,430  of  3,504.    Elapsed: 0:28:12.\n",
      "  Batch 2,440  of  3,504.    Elapsed: 0:28:19.\n",
      "  Batch 2,450  of  3,504.    Elapsed: 0:28:26.\n",
      "  Batch 2,460  of  3,504.    Elapsed: 0:28:33.\n",
      "  Batch 2,470  of  3,504.    Elapsed: 0:28:40.\n",
      "  Batch 2,480  of  3,504.    Elapsed: 0:28:47.\n",
      "  Batch 2,490  of  3,504.    Elapsed: 0:28:54.\n",
      "  Batch 2,500  of  3,504.    Elapsed: 0:29:01.\n",
      "  Batch 2,510  of  3,504.    Elapsed: 0:29:08.\n",
      "  Batch 2,520  of  3,504.    Elapsed: 0:29:14.\n",
      "  Batch 2,530  of  3,504.    Elapsed: 0:29:21.\n",
      "  Batch 2,540  of  3,504.    Elapsed: 0:29:28.\n",
      "  Batch 2,550  of  3,504.    Elapsed: 0:29:35.\n",
      "  Batch 2,560  of  3,504.    Elapsed: 0:29:42.\n",
      "  Batch 2,570  of  3,504.    Elapsed: 0:29:49.\n",
      "  Batch 2,580  of  3,504.    Elapsed: 0:29:56.\n",
      "  Batch 2,590  of  3,504.    Elapsed: 0:30:03.\n",
      "  Batch 2,600  of  3,504.    Elapsed: 0:30:10.\n",
      "  Batch 2,610  of  3,504.    Elapsed: 0:30:17.\n",
      "  Batch 2,620  of  3,504.    Elapsed: 0:30:24.\n",
      "  Batch 2,630  of  3,504.    Elapsed: 0:30:31.\n",
      "  Batch 2,640  of  3,504.    Elapsed: 0:30:38.\n",
      "  Batch 2,650  of  3,504.    Elapsed: 0:30:45.\n",
      "  Batch 2,660  of  3,504.    Elapsed: 0:30:52.\n",
      "  Batch 2,670  of  3,504.    Elapsed: 0:30:59.\n",
      "  Batch 2,680  of  3,504.    Elapsed: 0:31:06.\n",
      "  Batch 2,690  of  3,504.    Elapsed: 0:31:13.\n",
      "  Batch 2,700  of  3,504.    Elapsed: 0:31:20.\n",
      "  Batch 2,710  of  3,504.    Elapsed: 0:31:27.\n",
      "  Batch 2,720  of  3,504.    Elapsed: 0:31:34.\n",
      "  Batch 2,730  of  3,504.    Elapsed: 0:31:41.\n",
      "  Batch 2,740  of  3,504.    Elapsed: 0:31:48.\n",
      "  Batch 2,750  of  3,504.    Elapsed: 0:31:55.\n",
      "  Batch 2,760  of  3,504.    Elapsed: 0:32:01.\n",
      "  Batch 2,770  of  3,504.    Elapsed: 0:32:08.\n",
      "  Batch 2,780  of  3,504.    Elapsed: 0:32:15.\n",
      "  Batch 2,790  of  3,504.    Elapsed: 0:32:22.\n",
      "  Batch 2,800  of  3,504.    Elapsed: 0:32:29.\n",
      "  Batch 2,810  of  3,504.    Elapsed: 0:32:36.\n",
      "  Batch 2,820  of  3,504.    Elapsed: 0:32:43.\n",
      "  Batch 2,830  of  3,504.    Elapsed: 0:32:50.\n",
      "  Batch 2,840  of  3,504.    Elapsed: 0:32:57.\n",
      "  Batch 2,850  of  3,504.    Elapsed: 0:33:04.\n",
      "  Batch 2,860  of  3,504.    Elapsed: 0:33:11.\n",
      "  Batch 2,870  of  3,504.    Elapsed: 0:33:18.\n",
      "  Batch 2,880  of  3,504.    Elapsed: 0:33:25.\n",
      "  Batch 2,890  of  3,504.    Elapsed: 0:33:32.\n",
      "  Batch 2,900  of  3,504.    Elapsed: 0:33:39.\n",
      "  Batch 2,910  of  3,504.    Elapsed: 0:33:46.\n",
      "  Batch 2,920  of  3,504.    Elapsed: 0:33:53.\n",
      "  Batch 2,930  of  3,504.    Elapsed: 0:34:00.\n",
      "  Batch 2,940  of  3,504.    Elapsed: 0:34:07.\n",
      "  Batch 2,950  of  3,504.    Elapsed: 0:34:14.\n",
      "  Batch 2,960  of  3,504.    Elapsed: 0:34:21.\n",
      "  Batch 2,970  of  3,504.    Elapsed: 0:34:28.\n",
      "  Batch 2,980  of  3,504.    Elapsed: 0:34:35.\n",
      "  Batch 2,990  of  3,504.    Elapsed: 0:34:42.\n",
      "  Batch 3,000  of  3,504.    Elapsed: 0:34:49.\n",
      "  Batch 3,010  of  3,504.    Elapsed: 0:34:56.\n",
      "  Batch 3,020  of  3,504.    Elapsed: 0:35:02.\n",
      "  Batch 3,030  of  3,504.    Elapsed: 0:35:09.\n",
      "  Batch 3,040  of  3,504.    Elapsed: 0:35:16.\n",
      "  Batch 3,050  of  3,504.    Elapsed: 0:35:23.\n",
      "  Batch 3,060  of  3,504.    Elapsed: 0:35:30.\n",
      "  Batch 3,070  of  3,504.    Elapsed: 0:35:37.\n",
      "  Batch 3,080  of  3,504.    Elapsed: 0:35:44.\n",
      "  Batch 3,090  of  3,504.    Elapsed: 0:35:51.\n",
      "  Batch 3,100  of  3,504.    Elapsed: 0:35:58.\n",
      "  Batch 3,110  of  3,504.    Elapsed: 0:36:05.\n",
      "  Batch 3,120  of  3,504.    Elapsed: 0:36:12.\n",
      "  Batch 3,130  of  3,504.    Elapsed: 0:36:19.\n",
      "  Batch 3,140  of  3,504.    Elapsed: 0:36:26.\n",
      "  Batch 3,150  of  3,504.    Elapsed: 0:36:33.\n",
      "  Batch 3,160  of  3,504.    Elapsed: 0:36:40.\n",
      "  Batch 3,170  of  3,504.    Elapsed: 0:36:47.\n",
      "  Batch 3,180  of  3,504.    Elapsed: 0:36:54.\n",
      "  Batch 3,190  of  3,504.    Elapsed: 0:37:01.\n",
      "  Batch 3,200  of  3,504.    Elapsed: 0:37:08.\n",
      "  Batch 3,210  of  3,504.    Elapsed: 0:37:15.\n",
      "  Batch 3,220  of  3,504.    Elapsed: 0:37:22.\n",
      "  Batch 3,230  of  3,504.    Elapsed: 0:37:29.\n",
      "  Batch 3,240  of  3,504.    Elapsed: 0:37:35.\n",
      "  Batch 3,250  of  3,504.    Elapsed: 0:37:42.\n",
      "  Batch 3,260  of  3,504.    Elapsed: 0:37:49.\n",
      "  Batch 3,270  of  3,504.    Elapsed: 0:37:56.\n",
      "  Batch 3,280  of  3,504.    Elapsed: 0:38:03.\n",
      "  Batch 3,290  of  3,504.    Elapsed: 0:38:10.\n",
      "  Batch 3,300  of  3,504.    Elapsed: 0:38:17.\n",
      "  Batch 3,310  of  3,504.    Elapsed: 0:38:24.\n",
      "  Batch 3,320  of  3,504.    Elapsed: 0:38:31.\n",
      "  Batch 3,330  of  3,504.    Elapsed: 0:38:38.\n",
      "  Batch 3,340  of  3,504.    Elapsed: 0:38:45.\n",
      "  Batch 3,350  of  3,504.    Elapsed: 0:38:52.\n",
      "  Batch 3,360  of  3,504.    Elapsed: 0:38:59.\n",
      "  Batch 3,370  of  3,504.    Elapsed: 0:39:06.\n",
      "  Batch 3,380  of  3,504.    Elapsed: 0:39:13.\n",
      "  Batch 3,390  of  3,504.    Elapsed: 0:39:20.\n",
      "  Batch 3,400  of  3,504.    Elapsed: 0:39:27.\n",
      "  Batch 3,410  of  3,504.    Elapsed: 0:39:34.\n",
      "  Batch 3,420  of  3,504.    Elapsed: 0:39:41.\n",
      "  Batch 3,430  of  3,504.    Elapsed: 0:39:48.\n",
      "  Batch 3,440  of  3,504.    Elapsed: 0:39:55.\n",
      "  Batch 3,450  of  3,504.    Elapsed: 0:40:02.\n",
      "  Batch 3,460  of  3,504.    Elapsed: 0:40:09.\n",
      "  Batch 3,470  of  3,504.    Elapsed: 0:40:16.\n",
      "  Batch 3,480  of  3,504.    Elapsed: 0:40:23.\n",
      "  Batch 3,490  of  3,504.    Elapsed: 0:40:29.\n",
      "  Batch 3,500  of  3,504.    Elapsed: 0:40:36.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.882\n",
      "  Training epcoh took: 0:40:39\n",
      "\n",
      "Running Test...\n",
      "all_preds\n",
      "[4 1 1 ... 3 4 5]\n",
      "all_labels\n",
      "[3 1 1 ... 3 5 4]\n",
      "  Accuracy: 0.562\n",
      "  Test Loss: 1.766\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of  3,504.    Elapsed: 0:00:07.\n",
      "  Batch    20  of  3,504.    Elapsed: 0:00:14.\n",
      "  Batch    30  of  3,504.    Elapsed: 0:00:21.\n",
      "  Batch    40  of  3,504.    Elapsed: 0:00:28.\n",
      "  Batch    50  of  3,504.    Elapsed: 0:00:35.\n",
      "  Batch    60  of  3,504.    Elapsed: 0:00:42.\n",
      "  Batch    70  of  3,504.    Elapsed: 0:00:49.\n",
      "  Batch    80  of  3,504.    Elapsed: 0:00:56.\n",
      "  Batch    90  of  3,504.    Elapsed: 0:01:03.\n",
      "  Batch   100  of  3,504.    Elapsed: 0:01:10.\n",
      "  Batch   110  of  3,504.    Elapsed: 0:01:17.\n",
      "  Batch   120  of  3,504.    Elapsed: 0:01:24.\n",
      "  Batch   130  of  3,504.    Elapsed: 0:01:30.\n",
      "  Batch   140  of  3,504.    Elapsed: 0:01:37.\n",
      "  Batch   150  of  3,504.    Elapsed: 0:01:44.\n",
      "  Batch   160  of  3,504.    Elapsed: 0:01:51.\n",
      "  Batch   170  of  3,504.    Elapsed: 0:01:58.\n",
      "  Batch   180  of  3,504.    Elapsed: 0:02:05.\n",
      "  Batch   190  of  3,504.    Elapsed: 0:02:12.\n",
      "  Batch   200  of  3,504.    Elapsed: 0:02:19.\n",
      "  Batch   210  of  3,504.    Elapsed: 0:02:26.\n",
      "  Batch   220  of  3,504.    Elapsed: 0:02:33.\n",
      "  Batch   230  of  3,504.    Elapsed: 0:02:40.\n",
      "  Batch   240  of  3,504.    Elapsed: 0:02:47.\n",
      "  Batch   250  of  3,504.    Elapsed: 0:02:54.\n",
      "  Batch   260  of  3,504.    Elapsed: 0:03:01.\n",
      "  Batch   270  of  3,504.    Elapsed: 0:03:08.\n",
      "  Batch   280  of  3,504.    Elapsed: 0:03:15.\n",
      "  Batch   290  of  3,504.    Elapsed: 0:03:22.\n",
      "  Batch   300  of  3,504.    Elapsed: 0:03:29.\n",
      "  Batch   310  of  3,504.    Elapsed: 0:03:36.\n",
      "  Batch   320  of  3,504.    Elapsed: 0:03:43.\n",
      "  Batch   330  of  3,504.    Elapsed: 0:03:50.\n",
      "  Batch   340  of  3,504.    Elapsed: 0:03:57.\n",
      "  Batch   350  of  3,504.    Elapsed: 0:04:04.\n",
      "  Batch   360  of  3,504.    Elapsed: 0:04:11.\n",
      "  Batch   370  of  3,504.    Elapsed: 0:04:18.\n",
      "  Batch   380  of  3,504.    Elapsed: 0:04:25.\n",
      "  Batch   390  of  3,504.    Elapsed: 0:04:31.\n",
      "  Batch   400  of  3,504.    Elapsed: 0:04:38.\n",
      "  Batch   410  of  3,504.    Elapsed: 0:04:45.\n",
      "  Batch   420  of  3,504.    Elapsed: 0:04:52.\n",
      "  Batch   430  of  3,504.    Elapsed: 0:04:59.\n",
      "  Batch   440  of  3,504.    Elapsed: 0:05:06.\n",
      "  Batch   450  of  3,504.    Elapsed: 0:05:13.\n",
      "  Batch   460  of  3,504.    Elapsed: 0:05:20.\n",
      "  Batch   470  of  3,504.    Elapsed: 0:05:27.\n",
      "  Batch   480  of  3,504.    Elapsed: 0:05:34.\n",
      "  Batch   490  of  3,504.    Elapsed: 0:05:41.\n",
      "  Batch   500  of  3,504.    Elapsed: 0:05:48.\n",
      "  Batch   510  of  3,504.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  3,504.    Elapsed: 0:06:02.\n",
      "  Batch   530  of  3,504.    Elapsed: 0:06:09.\n",
      "  Batch   540  of  3,504.    Elapsed: 0:06:16.\n",
      "  Batch   550  of  3,504.    Elapsed: 0:06:23.\n",
      "  Batch   560  of  3,504.    Elapsed: 0:06:30.\n",
      "  Batch   570  of  3,504.    Elapsed: 0:06:37.\n",
      "  Batch   580  of  3,504.    Elapsed: 0:06:44.\n",
      "  Batch   590  of  3,504.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  3,504.    Elapsed: 0:06:58.\n",
      "  Batch   610  of  3,504.    Elapsed: 0:07:05.\n",
      "  Batch   620  of  3,504.    Elapsed: 0:07:12.\n",
      "  Batch   630  of  3,504.    Elapsed: 0:07:19.\n",
      "  Batch   640  of  3,504.    Elapsed: 0:07:26.\n",
      "  Batch   650  of  3,504.    Elapsed: 0:07:33.\n",
      "  Batch   660  of  3,504.    Elapsed: 0:07:39.\n",
      "  Batch   670  of  3,504.    Elapsed: 0:07:46.\n",
      "  Batch   680  of  3,504.    Elapsed: 0:07:53.\n",
      "  Batch   690  of  3,504.    Elapsed: 0:08:00.\n",
      "  Batch   700  of  3,504.    Elapsed: 0:08:07.\n",
      "  Batch   710  of  3,504.    Elapsed: 0:08:14.\n",
      "  Batch   720  of  3,504.    Elapsed: 0:08:21.\n",
      "  Batch   730  of  3,504.    Elapsed: 0:08:28.\n",
      "  Batch   740  of  3,504.    Elapsed: 0:08:35.\n",
      "  Batch   750  of  3,504.    Elapsed: 0:08:42.\n",
      "  Batch   760  of  3,504.    Elapsed: 0:08:49.\n",
      "  Batch   770  of  3,504.    Elapsed: 0:08:56.\n",
      "  Batch   780  of  3,504.    Elapsed: 0:09:03.\n",
      "  Batch   790  of  3,504.    Elapsed: 0:09:10.\n",
      "  Batch   800  of  3,504.    Elapsed: 0:09:17.\n",
      "  Batch   810  of  3,504.    Elapsed: 0:09:24.\n",
      "  Batch   820  of  3,504.    Elapsed: 0:09:31.\n",
      "  Batch   830  of  3,504.    Elapsed: 0:09:38.\n",
      "  Batch   840  of  3,504.    Elapsed: 0:09:45.\n",
      "  Batch   850  of  3,504.    Elapsed: 0:09:52.\n",
      "  Batch   860  of  3,504.    Elapsed: 0:09:59.\n",
      "  Batch   870  of  3,504.    Elapsed: 0:10:06.\n",
      "  Batch   880  of  3,504.    Elapsed: 0:10:13.\n",
      "  Batch   890  of  3,504.    Elapsed: 0:10:20.\n",
      "  Batch   900  of  3,504.    Elapsed: 0:10:27.\n",
      "  Batch   910  of  3,504.    Elapsed: 0:10:34.\n",
      "  Batch   920  of  3,504.    Elapsed: 0:10:41.\n",
      "  Batch   930  of  3,504.    Elapsed: 0:10:48.\n",
      "  Batch   940  of  3,504.    Elapsed: 0:10:54.\n",
      "  Batch   950  of  3,504.    Elapsed: 0:11:01.\n",
      "  Batch   960  of  3,504.    Elapsed: 0:11:08.\n",
      "  Batch   970  of  3,504.    Elapsed: 0:11:15.\n",
      "  Batch   980  of  3,504.    Elapsed: 0:11:22.\n",
      "  Batch   990  of  3,504.    Elapsed: 0:11:29.\n",
      "  Batch 1,000  of  3,504.    Elapsed: 0:11:36.\n",
      "  Batch 1,010  of  3,504.    Elapsed: 0:11:43.\n",
      "  Batch 1,020  of  3,504.    Elapsed: 0:11:50.\n",
      "  Batch 1,030  of  3,504.    Elapsed: 0:11:57.\n",
      "  Batch 1,040  of  3,504.    Elapsed: 0:12:04.\n",
      "  Batch 1,050  of  3,504.    Elapsed: 0:12:11.\n",
      "  Batch 1,060  of  3,504.    Elapsed: 0:12:18.\n",
      "  Batch 1,070  of  3,504.    Elapsed: 0:12:25.\n",
      "  Batch 1,080  of  3,504.    Elapsed: 0:12:32.\n",
      "  Batch 1,090  of  3,504.    Elapsed: 0:12:39.\n",
      "  Batch 1,100  of  3,504.    Elapsed: 0:12:46.\n",
      "  Batch 1,110  of  3,504.    Elapsed: 0:12:53.\n",
      "  Batch 1,120  of  3,504.    Elapsed: 0:13:00.\n",
      "  Batch 1,130  of  3,504.    Elapsed: 0:13:07.\n",
      "  Batch 1,140  of  3,504.    Elapsed: 0:13:14.\n",
      "  Batch 1,150  of  3,504.    Elapsed: 0:13:21.\n",
      "  Batch 1,160  of  3,504.    Elapsed: 0:13:28.\n",
      "  Batch 1,170  of  3,504.    Elapsed: 0:13:34.\n",
      "  Batch 1,180  of  3,504.    Elapsed: 0:13:41.\n",
      "  Batch 1,190  of  3,504.    Elapsed: 0:13:48.\n",
      "  Batch 1,200  of  3,504.    Elapsed: 0:13:55.\n",
      "  Batch 1,210  of  3,504.    Elapsed: 0:14:02.\n",
      "  Batch 1,220  of  3,504.    Elapsed: 0:14:09.\n",
      "  Batch 1,230  of  3,504.    Elapsed: 0:14:16.\n",
      "  Batch 1,240  of  3,504.    Elapsed: 0:14:23.\n",
      "  Batch 1,250  of  3,504.    Elapsed: 0:14:30.\n",
      "  Batch 1,260  of  3,504.    Elapsed: 0:14:37.\n",
      "  Batch 1,270  of  3,504.    Elapsed: 0:14:44.\n",
      "  Batch 1,280  of  3,504.    Elapsed: 0:14:51.\n",
      "  Batch 1,290  of  3,504.    Elapsed: 0:14:58.\n",
      "  Batch 1,300  of  3,504.    Elapsed: 0:15:05.\n",
      "  Batch 1,310  of  3,504.    Elapsed: 0:15:12.\n",
      "  Batch 1,320  of  3,504.    Elapsed: 0:15:19.\n",
      "  Batch 1,330  of  3,504.    Elapsed: 0:15:26.\n",
      "  Batch 1,340  of  3,504.    Elapsed: 0:15:33.\n",
      "  Batch 1,350  of  3,504.    Elapsed: 0:15:40.\n",
      "  Batch 1,360  of  3,504.    Elapsed: 0:15:47.\n",
      "  Batch 1,370  of  3,504.    Elapsed: 0:15:54.\n",
      "  Batch 1,380  of  3,504.    Elapsed: 0:16:01.\n",
      "  Batch 1,390  of  3,504.    Elapsed: 0:16:08.\n",
      "  Batch 1,400  of  3,504.    Elapsed: 0:16:15.\n",
      "  Batch 1,410  of  3,504.    Elapsed: 0:16:22.\n",
      "  Batch 1,420  of  3,504.    Elapsed: 0:16:29.\n",
      "  Batch 1,430  of  3,504.    Elapsed: 0:16:36.\n",
      "  Batch 1,440  of  3,504.    Elapsed: 0:16:43.\n",
      "  Batch 1,450  of  3,504.    Elapsed: 0:16:50.\n",
      "  Batch 1,460  of  3,504.    Elapsed: 0:16:57.\n",
      "  Batch 1,470  of  3,504.    Elapsed: 0:17:04.\n",
      "  Batch 1,480  of  3,504.    Elapsed: 0:17:11.\n",
      "  Batch 1,490  of  3,504.    Elapsed: 0:17:17.\n",
      "  Batch 1,500  of  3,504.    Elapsed: 0:17:24.\n",
      "  Batch 1,510  of  3,504.    Elapsed: 0:17:31.\n",
      "  Batch 1,520  of  3,504.    Elapsed: 0:17:38.\n",
      "  Batch 1,530  of  3,504.    Elapsed: 0:17:45.\n",
      "  Batch 1,540  of  3,504.    Elapsed: 0:17:52.\n",
      "  Batch 1,550  of  3,504.    Elapsed: 0:17:59.\n",
      "  Batch 1,560  of  3,504.    Elapsed: 0:18:06.\n",
      "  Batch 1,570  of  3,504.    Elapsed: 0:18:13.\n",
      "  Batch 1,580  of  3,504.    Elapsed: 0:18:20.\n",
      "  Batch 1,590  of  3,504.    Elapsed: 0:18:27.\n",
      "  Batch 1,600  of  3,504.    Elapsed: 0:18:34.\n",
      "  Batch 1,610  of  3,504.    Elapsed: 0:18:41.\n",
      "  Batch 1,620  of  3,504.    Elapsed: 0:18:48.\n",
      "  Batch 1,630  of  3,504.    Elapsed: 0:18:55.\n",
      "  Batch 1,640  of  3,504.    Elapsed: 0:19:02.\n",
      "  Batch 1,650  of  3,504.    Elapsed: 0:19:09.\n",
      "  Batch 1,660  of  3,504.    Elapsed: 0:19:16.\n",
      "  Batch 1,670  of  3,504.    Elapsed: 0:19:23.\n",
      "  Batch 1,680  of  3,504.    Elapsed: 0:19:30.\n",
      "  Batch 1,690  of  3,504.    Elapsed: 0:19:37.\n",
      "  Batch 1,700  of  3,504.    Elapsed: 0:19:44.\n",
      "  Batch 1,710  of  3,504.    Elapsed: 0:19:51.\n",
      "  Batch 1,720  of  3,504.    Elapsed: 0:19:58.\n",
      "  Batch 1,730  of  3,504.    Elapsed: 0:20:05.\n",
      "  Batch 1,740  of  3,504.    Elapsed: 0:20:12.\n",
      "  Batch 1,750  of  3,504.    Elapsed: 0:20:19.\n",
      "  Batch 1,760  of  3,504.    Elapsed: 0:20:25.\n",
      "  Batch 1,770  of  3,504.    Elapsed: 0:20:32.\n",
      "  Batch 1,780  of  3,504.    Elapsed: 0:20:39.\n",
      "  Batch 1,790  of  3,504.    Elapsed: 0:20:46.\n",
      "  Batch 1,800  of  3,504.    Elapsed: 0:20:53.\n",
      "  Batch 1,810  of  3,504.    Elapsed: 0:21:00.\n",
      "  Batch 1,820  of  3,504.    Elapsed: 0:21:07.\n",
      "  Batch 1,830  of  3,504.    Elapsed: 0:21:14.\n",
      "  Batch 1,840  of  3,504.    Elapsed: 0:21:21.\n",
      "  Batch 1,850  of  3,504.    Elapsed: 0:21:28.\n",
      "  Batch 1,860  of  3,504.    Elapsed: 0:21:35.\n",
      "  Batch 1,870  of  3,504.    Elapsed: 0:21:42.\n",
      "  Batch 1,880  of  3,504.    Elapsed: 0:21:49.\n",
      "  Batch 1,890  of  3,504.    Elapsed: 0:21:56.\n",
      "  Batch 1,900  of  3,504.    Elapsed: 0:22:03.\n",
      "  Batch 1,910  of  3,504.    Elapsed: 0:22:10.\n",
      "  Batch 1,920  of  3,504.    Elapsed: 0:22:17.\n",
      "  Batch 1,930  of  3,504.    Elapsed: 0:22:24.\n",
      "  Batch 1,940  of  3,504.    Elapsed: 0:22:31.\n",
      "  Batch 1,950  of  3,504.    Elapsed: 0:22:38.\n",
      "  Batch 1,960  of  3,504.    Elapsed: 0:22:45.\n",
      "  Batch 1,970  of  3,504.    Elapsed: 0:22:52.\n",
      "  Batch 1,980  of  3,504.    Elapsed: 0:22:59.\n",
      "  Batch 1,990  of  3,504.    Elapsed: 0:23:05.\n",
      "  Batch 2,000  of  3,504.    Elapsed: 0:23:12.\n",
      "  Batch 2,010  of  3,504.    Elapsed: 0:23:19.\n",
      "  Batch 2,020  of  3,504.    Elapsed: 0:23:26.\n",
      "  Batch 2,030  of  3,504.    Elapsed: 0:23:33.\n",
      "  Batch 2,040  of  3,504.    Elapsed: 0:23:40.\n",
      "  Batch 2,050  of  3,504.    Elapsed: 0:23:47.\n",
      "  Batch 2,060  of  3,504.    Elapsed: 0:23:54.\n",
      "  Batch 2,070  of  3,504.    Elapsed: 0:24:01.\n",
      "  Batch 2,080  of  3,504.    Elapsed: 0:24:08.\n",
      "  Batch 2,090  of  3,504.    Elapsed: 0:24:15.\n",
      "  Batch 2,100  of  3,504.    Elapsed: 0:24:22.\n",
      "  Batch 2,110  of  3,504.    Elapsed: 0:24:29.\n",
      "  Batch 2,120  of  3,504.    Elapsed: 0:24:36.\n",
      "  Batch 2,130  of  3,504.    Elapsed: 0:24:43.\n",
      "  Batch 2,140  of  3,504.    Elapsed: 0:24:50.\n",
      "  Batch 2,150  of  3,504.    Elapsed: 0:24:57.\n",
      "  Batch 2,160  of  3,504.    Elapsed: 0:25:04.\n",
      "  Batch 2,170  of  3,504.    Elapsed: 0:25:11.\n",
      "  Batch 2,180  of  3,504.    Elapsed: 0:25:18.\n",
      "  Batch 2,190  of  3,504.    Elapsed: 0:25:25.\n",
      "  Batch 2,200  of  3,504.    Elapsed: 0:25:32.\n",
      "  Batch 2,210  of  3,504.    Elapsed: 0:25:39.\n",
      "  Batch 2,220  of  3,504.    Elapsed: 0:25:45.\n",
      "  Batch 2,230  of  3,504.    Elapsed: 0:25:52.\n",
      "  Batch 2,240  of  3,504.    Elapsed: 0:25:59.\n",
      "  Batch 2,250  of  3,504.    Elapsed: 0:26:06.\n",
      "  Batch 2,260  of  3,504.    Elapsed: 0:26:13.\n",
      "  Batch 2,270  of  3,504.    Elapsed: 0:26:20.\n",
      "  Batch 2,280  of  3,504.    Elapsed: 0:26:27.\n",
      "  Batch 2,290  of  3,504.    Elapsed: 0:26:34.\n",
      "  Batch 2,300  of  3,504.    Elapsed: 0:26:41.\n",
      "  Batch 2,310  of  3,504.    Elapsed: 0:26:48.\n",
      "  Batch 2,320  of  3,504.    Elapsed: 0:26:55.\n",
      "  Batch 2,330  of  3,504.    Elapsed: 0:27:02.\n",
      "  Batch 2,340  of  3,504.    Elapsed: 0:27:09.\n",
      "  Batch 2,350  of  3,504.    Elapsed: 0:27:16.\n",
      "  Batch 2,360  of  3,504.    Elapsed: 0:27:23.\n",
      "  Batch 2,370  of  3,504.    Elapsed: 0:27:30.\n",
      "  Batch 2,380  of  3,504.    Elapsed: 0:27:37.\n",
      "  Batch 2,390  of  3,504.    Elapsed: 0:27:44.\n",
      "  Batch 2,400  of  3,504.    Elapsed: 0:27:51.\n",
      "  Batch 2,410  of  3,504.    Elapsed: 0:27:58.\n",
      "  Batch 2,420  of  3,504.    Elapsed: 0:28:05.\n",
      "  Batch 2,430  of  3,504.    Elapsed: 0:28:12.\n",
      "  Batch 2,440  of  3,504.    Elapsed: 0:28:19.\n",
      "  Batch 2,450  of  3,504.    Elapsed: 0:28:26.\n",
      "  Batch 2,460  of  3,504.    Elapsed: 0:28:32.\n",
      "  Batch 2,470  of  3,504.    Elapsed: 0:28:39.\n",
      "  Batch 2,480  of  3,504.    Elapsed: 0:28:46.\n",
      "  Batch 2,490  of  3,504.    Elapsed: 0:28:53.\n",
      "  Batch 2,500  of  3,504.    Elapsed: 0:29:00.\n",
      "  Batch 2,510  of  3,504.    Elapsed: 0:29:07.\n",
      "  Batch 2,520  of  3,504.    Elapsed: 0:29:14.\n",
      "  Batch 2,530  of  3,504.    Elapsed: 0:29:21.\n",
      "  Batch 2,540  of  3,504.    Elapsed: 0:29:28.\n",
      "  Batch 2,550  of  3,504.    Elapsed: 0:29:35.\n",
      "  Batch 2,560  of  3,504.    Elapsed: 0:29:42.\n",
      "  Batch 2,570  of  3,504.    Elapsed: 0:29:49.\n",
      "  Batch 2,580  of  3,504.    Elapsed: 0:29:56.\n",
      "  Batch 2,590  of  3,504.    Elapsed: 0:30:03.\n",
      "  Batch 2,600  of  3,504.    Elapsed: 0:30:10.\n",
      "  Batch 2,610  of  3,504.    Elapsed: 0:30:17.\n",
      "  Batch 2,620  of  3,504.    Elapsed: 0:30:24.\n",
      "  Batch 2,630  of  3,504.    Elapsed: 0:30:31.\n",
      "  Batch 2,640  of  3,504.    Elapsed: 0:30:38.\n",
      "  Batch 2,650  of  3,504.    Elapsed: 0:30:45.\n",
      "  Batch 2,660  of  3,504.    Elapsed: 0:30:52.\n",
      "  Batch 2,670  of  3,504.    Elapsed: 0:30:59.\n",
      "  Batch 2,680  of  3,504.    Elapsed: 0:31:06.\n",
      "  Batch 2,690  of  3,504.    Elapsed: 0:31:13.\n",
      "  Batch 2,700  of  3,504.    Elapsed: 0:31:20.\n",
      "  Batch 2,710  of  3,504.    Elapsed: 0:31:27.\n",
      "  Batch 2,720  of  3,504.    Elapsed: 0:31:33.\n",
      "  Batch 2,730  of  3,504.    Elapsed: 0:31:40.\n",
      "  Batch 2,740  of  3,504.    Elapsed: 0:31:47.\n",
      "  Batch 2,750  of  3,504.    Elapsed: 0:31:54.\n",
      "  Batch 2,760  of  3,504.    Elapsed: 0:32:01.\n",
      "  Batch 2,770  of  3,504.    Elapsed: 0:32:08.\n",
      "  Batch 2,780  of  3,504.    Elapsed: 0:32:15.\n",
      "  Batch 2,790  of  3,504.    Elapsed: 0:32:22.\n",
      "  Batch 2,800  of  3,504.    Elapsed: 0:32:29.\n",
      "  Batch 2,810  of  3,504.    Elapsed: 0:32:36.\n",
      "  Batch 2,820  of  3,504.    Elapsed: 0:32:43.\n",
      "  Batch 2,830  of  3,504.    Elapsed: 0:32:50.\n",
      "  Batch 2,840  of  3,504.    Elapsed: 0:32:57.\n",
      "  Batch 2,850  of  3,504.    Elapsed: 0:33:04.\n",
      "  Batch 2,860  of  3,504.    Elapsed: 0:33:11.\n",
      "  Batch 2,870  of  3,504.    Elapsed: 0:33:18.\n",
      "  Batch 2,880  of  3,504.    Elapsed: 0:33:25.\n",
      "  Batch 2,890  of  3,504.    Elapsed: 0:33:32.\n",
      "  Batch 2,900  of  3,504.    Elapsed: 0:33:39.\n",
      "  Batch 2,910  of  3,504.    Elapsed: 0:33:46.\n",
      "  Batch 2,920  of  3,504.    Elapsed: 0:33:53.\n",
      "  Batch 2,930  of  3,504.    Elapsed: 0:34:00.\n",
      "  Batch 2,940  of  3,504.    Elapsed: 0:34:07.\n",
      "  Batch 2,950  of  3,504.    Elapsed: 0:34:14.\n",
      "  Batch 2,960  of  3,504.    Elapsed: 0:34:21.\n",
      "  Batch 2,970  of  3,504.    Elapsed: 0:34:28.\n",
      "  Batch 2,980  of  3,504.    Elapsed: 0:34:35.\n",
      "  Batch 2,990  of  3,504.    Elapsed: 0:34:42.\n",
      "  Batch 3,000  of  3,504.    Elapsed: 0:34:49.\n",
      "  Batch 3,010  of  3,504.    Elapsed: 0:34:56.\n",
      "  Batch 3,020  of  3,504.    Elapsed: 0:35:02.\n",
      "  Batch 3,030  of  3,504.    Elapsed: 0:35:09.\n",
      "  Batch 3,040  of  3,504.    Elapsed: 0:35:16.\n",
      "  Batch 3,050  of  3,504.    Elapsed: 0:35:23.\n",
      "  Batch 3,060  of  3,504.    Elapsed: 0:35:30.\n",
      "  Batch 3,070  of  3,504.    Elapsed: 0:35:37.\n",
      "  Batch 3,080  of  3,504.    Elapsed: 0:35:44.\n",
      "  Batch 3,090  of  3,504.    Elapsed: 0:35:51.\n",
      "  Batch 3,100  of  3,504.    Elapsed: 0:35:58.\n",
      "  Batch 3,110  of  3,504.    Elapsed: 0:36:05.\n",
      "  Batch 3,120  of  3,504.    Elapsed: 0:36:12.\n",
      "  Batch 3,130  of  3,504.    Elapsed: 0:36:19.\n",
      "  Batch 3,140  of  3,504.    Elapsed: 0:36:26.\n",
      "  Batch 3,150  of  3,504.    Elapsed: 0:36:33.\n",
      "  Batch 3,160  of  3,504.    Elapsed: 0:36:40.\n",
      "  Batch 3,170  of  3,504.    Elapsed: 0:36:47.\n",
      "  Batch 3,180  of  3,504.    Elapsed: 0:36:54.\n",
      "  Batch 3,190  of  3,504.    Elapsed: 0:37:01.\n",
      "  Batch 3,200  of  3,504.    Elapsed: 0:37:08.\n",
      "  Batch 3,210  of  3,504.    Elapsed: 0:37:15.\n",
      "  Batch 3,220  of  3,504.    Elapsed: 0:37:22.\n",
      "  Batch 3,230  of  3,504.    Elapsed: 0:37:29.\n",
      "  Batch 3,240  of  3,504.    Elapsed: 0:37:36.\n",
      "  Batch 3,250  of  3,504.    Elapsed: 0:37:43.\n",
      "  Batch 3,260  of  3,504.    Elapsed: 0:37:50.\n",
      "  Batch 3,270  of  3,504.    Elapsed: 0:37:56.\n",
      "  Batch 3,280  of  3,504.    Elapsed: 0:38:03.\n",
      "  Batch 3,290  of  3,504.    Elapsed: 0:38:10.\n",
      "  Batch 3,300  of  3,504.    Elapsed: 0:38:17.\n",
      "  Batch 3,310  of  3,504.    Elapsed: 0:38:24.\n",
      "  Batch 3,320  of  3,504.    Elapsed: 0:38:31.\n",
      "  Batch 3,330  of  3,504.    Elapsed: 0:38:38.\n",
      "  Batch 3,340  of  3,504.    Elapsed: 0:38:45.\n",
      "  Batch 3,350  of  3,504.    Elapsed: 0:38:52.\n",
      "  Batch 3,360  of  3,504.    Elapsed: 0:38:59.\n",
      "  Batch 3,370  of  3,504.    Elapsed: 0:39:06.\n",
      "  Batch 3,380  of  3,504.    Elapsed: 0:39:13.\n",
      "  Batch 3,390  of  3,504.    Elapsed: 0:39:20.\n",
      "  Batch 3,400  of  3,504.    Elapsed: 0:39:27.\n",
      "  Batch 3,410  of  3,504.    Elapsed: 0:39:34.\n",
      "  Batch 3,420  of  3,504.    Elapsed: 0:39:41.\n",
      "  Batch 3,430  of  3,504.    Elapsed: 0:39:48.\n",
      "  Batch 3,440  of  3,504.    Elapsed: 0:39:55.\n",
      "  Batch 3,450  of  3,504.    Elapsed: 0:40:02.\n",
      "  Batch 3,460  of  3,504.    Elapsed: 0:40:09.\n",
      "  Batch 3,470  of  3,504.    Elapsed: 0:40:16.\n",
      "  Batch 3,480  of  3,504.    Elapsed: 0:40:23.\n",
      "  Batch 3,490  of  3,504.    Elapsed: 0:40:30.\n",
      "  Batch 3,500  of  3,504.    Elapsed: 0:40:37.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.834\n",
      "  Training epcoh took: 0:40:39\n",
      "\n",
      "Running Test...\n",
      "all_preds\n",
      "[4 1 1 ... 3 4 4]\n",
      "all_labels\n",
      "[3 1 1 ... 3 5 4]\n",
      "  Accuracy: 0.538\n",
      "  Test Loss: 2.238\n",
      "  Test took: 0:00:12\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of  3,504.    Elapsed: 0:00:07.\n",
      "  Batch    20  of  3,504.    Elapsed: 0:00:14.\n",
      "  Batch    30  of  3,504.    Elapsed: 0:00:21.\n",
      "  Batch    40  of  3,504.    Elapsed: 0:00:28.\n",
      "  Batch    50  of  3,504.    Elapsed: 0:00:35.\n",
      "  Batch    60  of  3,504.    Elapsed: 0:00:42.\n",
      "  Batch    70  of  3,504.    Elapsed: 0:00:49.\n",
      "  Batch    80  of  3,504.    Elapsed: 0:00:56.\n",
      "  Batch    90  of  3,504.    Elapsed: 0:01:03.\n",
      "  Batch   100  of  3,504.    Elapsed: 0:01:10.\n",
      "  Batch   110  of  3,504.    Elapsed: 0:01:17.\n",
      "  Batch   120  of  3,504.    Elapsed: 0:01:24.\n",
      "  Batch   130  of  3,504.    Elapsed: 0:01:31.\n",
      "  Batch   140  of  3,504.    Elapsed: 0:01:38.\n",
      "  Batch   150  of  3,504.    Elapsed: 0:01:44.\n",
      "  Batch   160  of  3,504.    Elapsed: 0:01:51.\n",
      "  Batch   170  of  3,504.    Elapsed: 0:01:58.\n",
      "  Batch   180  of  3,504.    Elapsed: 0:02:05.\n",
      "  Batch   190  of  3,504.    Elapsed: 0:02:12.\n",
      "  Batch   200  of  3,504.    Elapsed: 0:02:19.\n",
      "  Batch   210  of  3,504.    Elapsed: 0:02:26.\n",
      "  Batch   220  of  3,504.    Elapsed: 0:02:33.\n",
      "  Batch   230  of  3,504.    Elapsed: 0:02:40.\n",
      "  Batch   240  of  3,504.    Elapsed: 0:02:47.\n",
      "  Batch   250  of  3,504.    Elapsed: 0:02:54.\n",
      "  Batch   260  of  3,504.    Elapsed: 0:03:01.\n",
      "  Batch   270  of  3,504.    Elapsed: 0:03:08.\n",
      "  Batch   280  of  3,504.    Elapsed: 0:03:15.\n",
      "  Batch   290  of  3,504.    Elapsed: 0:03:22.\n",
      "  Batch   300  of  3,504.    Elapsed: 0:03:29.\n",
      "  Batch   310  of  3,504.    Elapsed: 0:03:36.\n",
      "  Batch   320  of  3,504.    Elapsed: 0:03:43.\n",
      "  Batch   330  of  3,504.    Elapsed: 0:03:50.\n",
      "  Batch   340  of  3,504.    Elapsed: 0:03:57.\n",
      "  Batch   350  of  3,504.    Elapsed: 0:04:04.\n",
      "  Batch   360  of  3,504.    Elapsed: 0:04:11.\n",
      "  Batch   370  of  3,504.    Elapsed: 0:04:18.\n",
      "  Batch   380  of  3,504.    Elapsed: 0:04:25.\n",
      "  Batch   390  of  3,504.    Elapsed: 0:04:32.\n",
      "  Batch   400  of  3,504.    Elapsed: 0:04:39.\n",
      "  Batch   410  of  3,504.    Elapsed: 0:04:46.\n",
      "  Batch   420  of  3,504.    Elapsed: 0:04:53.\n",
      "  Batch   430  of  3,504.    Elapsed: 0:05:00.\n",
      "  Batch   440  of  3,504.    Elapsed: 0:05:07.\n",
      "  Batch   450  of  3,504.    Elapsed: 0:05:14.\n",
      "  Batch   460  of  3,504.    Elapsed: 0:05:21.\n",
      "  Batch   470  of  3,504.    Elapsed: 0:05:27.\n",
      "  Batch   480  of  3,504.    Elapsed: 0:05:34.\n",
      "  Batch   490  of  3,504.    Elapsed: 0:05:41.\n",
      "  Batch   500  of  3,504.    Elapsed: 0:05:48.\n",
      "  Batch   510  of  3,504.    Elapsed: 0:05:55.\n",
      "  Batch   520  of  3,504.    Elapsed: 0:06:02.\n",
      "  Batch   530  of  3,504.    Elapsed: 0:06:09.\n",
      "  Batch   540  of  3,504.    Elapsed: 0:06:16.\n",
      "  Batch   550  of  3,504.    Elapsed: 0:06:23.\n",
      "  Batch   560  of  3,504.    Elapsed: 0:06:30.\n",
      "  Batch   570  of  3,504.    Elapsed: 0:06:37.\n",
      "  Batch   580  of  3,504.    Elapsed: 0:06:44.\n",
      "  Batch   590  of  3,504.    Elapsed: 0:06:51.\n",
      "  Batch   600  of  3,504.    Elapsed: 0:06:58.\n",
      "  Batch   610  of  3,504.    Elapsed: 0:07:05.\n",
      "  Batch   620  of  3,504.    Elapsed: 0:07:12.\n",
      "  Batch   630  of  3,504.    Elapsed: 0:07:19.\n",
      "  Batch   640  of  3,504.    Elapsed: 0:07:26.\n",
      "  Batch   650  of  3,504.    Elapsed: 0:07:33.\n",
      "  Batch   660  of  3,504.    Elapsed: 0:07:40.\n",
      "  Batch   670  of  3,504.    Elapsed: 0:07:47.\n",
      "  Batch   680  of  3,504.    Elapsed: 0:07:54.\n",
      "  Batch   690  of  3,504.    Elapsed: 0:08:01.\n",
      "  Batch   700  of  3,504.    Elapsed: 0:08:07.\n",
      "  Batch   710  of  3,504.    Elapsed: 0:08:14.\n",
      "  Batch   720  of  3,504.    Elapsed: 0:08:21.\n",
      "  Batch   730  of  3,504.    Elapsed: 0:08:28.\n",
      "  Batch   740  of  3,504.    Elapsed: 0:08:35.\n",
      "  Batch   750  of  3,504.    Elapsed: 0:08:42.\n",
      "  Batch   760  of  3,504.    Elapsed: 0:08:49.\n",
      "  Batch   770  of  3,504.    Elapsed: 0:08:56.\n",
      "  Batch   780  of  3,504.    Elapsed: 0:09:03.\n",
      "  Batch   790  of  3,504.    Elapsed: 0:09:10.\n",
      "  Batch   800  of  3,504.    Elapsed: 0:09:17.\n",
      "  Batch   810  of  3,504.    Elapsed: 0:09:24.\n",
      "  Batch   820  of  3,504.    Elapsed: 0:09:31.\n",
      "  Batch   830  of  3,504.    Elapsed: 0:09:38.\n",
      "  Batch   840  of  3,504.    Elapsed: 0:09:45.\n",
      "  Batch   850  of  3,504.    Elapsed: 0:09:52.\n",
      "  Batch   860  of  3,504.    Elapsed: 0:09:59.\n",
      "  Batch   870  of  3,504.    Elapsed: 0:10:06.\n",
      "  Batch   880  of  3,504.    Elapsed: 0:10:13.\n",
      "  Batch   890  of  3,504.    Elapsed: 0:10:20.\n",
      "  Batch   900  of  3,504.    Elapsed: 0:10:27.\n",
      "  Batch   910  of  3,504.    Elapsed: 0:10:34.\n",
      "  Batch   920  of  3,504.    Elapsed: 0:10:41.\n",
      "  Batch   930  of  3,504.    Elapsed: 0:10:48.\n",
      "  Batch   940  of  3,504.    Elapsed: 0:10:55.\n",
      "  Batch   950  of  3,504.    Elapsed: 0:11:02.\n",
      "  Batch   960  of  3,504.    Elapsed: 0:11:09.\n",
      "  Batch   970  of  3,504.    Elapsed: 0:11:16.\n",
      "  Batch   980  of  3,504.    Elapsed: 0:11:23.\n",
      "  Batch   990  of  3,504.    Elapsed: 0:11:30.\n",
      "  Batch 1,000  of  3,504.    Elapsed: 0:11:37.\n",
      "  Batch 1,010  of  3,504.    Elapsed: 0:11:44.\n",
      "  Batch 1,020  of  3,504.    Elapsed: 0:11:51.\n",
      "  Batch 1,030  of  3,504.    Elapsed: 0:11:57.\n",
      "  Batch 1,040  of  3,504.    Elapsed: 0:12:04.\n",
      "  Batch 1,050  of  3,504.    Elapsed: 0:12:11.\n",
      "  Batch 1,060  of  3,504.    Elapsed: 0:12:18.\n",
      "  Batch 1,070  of  3,504.    Elapsed: 0:12:25.\n",
      "  Batch 1,080  of  3,504.    Elapsed: 0:12:32.\n",
      "  Batch 1,090  of  3,504.    Elapsed: 0:12:39.\n",
      "  Batch 1,100  of  3,504.    Elapsed: 0:12:46.\n",
      "  Batch 1,110  of  3,504.    Elapsed: 0:12:53.\n",
      "  Batch 1,120  of  3,504.    Elapsed: 0:13:00.\n",
      "  Batch 1,130  of  3,504.    Elapsed: 0:13:07.\n",
      "  Batch 1,140  of  3,504.    Elapsed: 0:13:14.\n",
      "  Batch 1,150  of  3,504.    Elapsed: 0:13:21.\n",
      "  Batch 1,160  of  3,504.    Elapsed: 0:13:28.\n",
      "  Batch 1,170  of  3,504.    Elapsed: 0:13:35.\n",
      "  Batch 1,180  of  3,504.    Elapsed: 0:13:42.\n",
      "  Batch 1,190  of  3,504.    Elapsed: 0:13:49.\n",
      "  Batch 1,200  of  3,504.    Elapsed: 0:13:56.\n",
      "  Batch 1,210  of  3,504.    Elapsed: 0:14:03.\n",
      "  Batch 1,220  of  3,504.    Elapsed: 0:14:10.\n",
      "  Batch 1,230  of  3,504.    Elapsed: 0:14:17.\n",
      "  Batch 1,240  of  3,504.    Elapsed: 0:14:24.\n",
      "  Batch 1,250  of  3,504.    Elapsed: 0:14:31.\n",
      "  Batch 1,260  of  3,504.    Elapsed: 0:14:38.\n",
      "  Batch 1,270  of  3,504.    Elapsed: 0:14:45.\n",
      "  Batch 1,280  of  3,504.    Elapsed: 0:14:51.\n",
      "  Batch 1,290  of  3,504.    Elapsed: 0:14:58.\n",
      "  Batch 1,300  of  3,504.    Elapsed: 0:15:05.\n",
      "  Batch 1,310  of  3,504.    Elapsed: 0:15:12.\n",
      "  Batch 1,320  of  3,504.    Elapsed: 0:15:19.\n",
      "  Batch 1,330  of  3,504.    Elapsed: 0:15:26.\n",
      "  Batch 1,340  of  3,504.    Elapsed: 0:15:33.\n",
      "  Batch 1,350  of  3,504.    Elapsed: 0:15:40.\n",
      "  Batch 1,360  of  3,504.    Elapsed: 0:15:47.\n",
      "  Batch 1,370  of  3,504.    Elapsed: 0:15:54.\n",
      "  Batch 1,380  of  3,504.    Elapsed: 0:16:01.\n",
      "  Batch 1,390  of  3,504.    Elapsed: 0:16:08.\n",
      "  Batch 1,400  of  3,504.    Elapsed: 0:16:15.\n",
      "  Batch 1,410  of  3,504.    Elapsed: 0:16:22.\n",
      "  Batch 1,420  of  3,504.    Elapsed: 0:16:29.\n",
      "  Batch 1,430  of  3,504.    Elapsed: 0:16:36.\n",
      "  Batch 1,440  of  3,504.    Elapsed: 0:16:43.\n",
      "  Batch 1,450  of  3,504.    Elapsed: 0:16:50.\n",
      "  Batch 1,460  of  3,504.    Elapsed: 0:16:57.\n",
      "  Batch 1,470  of  3,504.    Elapsed: 0:17:04.\n",
      "  Batch 1,480  of  3,504.    Elapsed: 0:17:11.\n",
      "  Batch 1,490  of  3,504.    Elapsed: 0:17:18.\n",
      "  Batch 1,500  of  3,504.    Elapsed: 0:17:25.\n",
      "  Batch 1,510  of  3,504.    Elapsed: 0:17:32.\n",
      "  Batch 1,520  of  3,504.    Elapsed: 0:17:39.\n",
      "  Batch 1,530  of  3,504.    Elapsed: 0:17:45.\n",
      "  Batch 1,540  of  3,504.    Elapsed: 0:17:52.\n",
      "  Batch 1,550  of  3,504.    Elapsed: 0:17:59.\n",
      "  Batch 1,560  of  3,504.    Elapsed: 0:18:06.\n",
      "  Batch 1,570  of  3,504.    Elapsed: 0:18:13.\n",
      "  Batch 1,580  of  3,504.    Elapsed: 0:18:20.\n",
      "  Batch 1,590  of  3,504.    Elapsed: 0:18:27.\n",
      "  Batch 1,600  of  3,504.    Elapsed: 0:18:34.\n",
      "  Batch 1,610  of  3,504.    Elapsed: 0:18:41.\n",
      "  Batch 1,620  of  3,504.    Elapsed: 0:18:48.\n",
      "  Batch 1,630  of  3,504.    Elapsed: 0:18:55.\n",
      "  Batch 1,640  of  3,504.    Elapsed: 0:19:02.\n",
      "  Batch 1,650  of  3,504.    Elapsed: 0:19:09.\n",
      "  Batch 1,660  of  3,504.    Elapsed: 0:19:16.\n",
      "  Batch 1,670  of  3,504.    Elapsed: 0:19:23.\n",
      "  Batch 1,680  of  3,504.    Elapsed: 0:19:30.\n",
      "  Batch 1,690  of  3,504.    Elapsed: 0:19:37.\n",
      "  Batch 1,700  of  3,504.    Elapsed: 0:19:44.\n",
      "  Batch 1,710  of  3,504.    Elapsed: 0:19:51.\n",
      "  Batch 1,720  of  3,504.    Elapsed: 0:19:58.\n",
      "  Batch 1,730  of  3,504.    Elapsed: 0:20:05.\n",
      "  Batch 1,740  of  3,504.    Elapsed: 0:20:12.\n",
      "  Batch 1,750  of  3,504.    Elapsed: 0:20:19.\n",
      "  Batch 1,760  of  3,504.    Elapsed: 0:20:26.\n",
      "  Batch 1,770  of  3,504.    Elapsed: 0:20:33.\n",
      "  Batch 1,780  of  3,504.    Elapsed: 0:20:40.\n",
      "  Batch 1,790  of  3,504.    Elapsed: 0:20:47.\n",
      "  Batch 1,800  of  3,504.    Elapsed: 0:20:54.\n",
      "  Batch 1,810  of  3,504.    Elapsed: 0:21:01.\n",
      "  Batch 1,820  of  3,504.    Elapsed: 0:21:08.\n",
      "  Batch 1,830  of  3,504.    Elapsed: 0:21:15.\n",
      "  Batch 1,840  of  3,504.    Elapsed: 0:21:22.\n",
      "  Batch 1,850  of  3,504.    Elapsed: 0:21:29.\n",
      "  Batch 1,860  of  3,504.    Elapsed: 0:21:36.\n",
      "  Batch 1,870  of  3,504.    Elapsed: 0:21:43.\n",
      "  Batch 1,880  of  3,504.    Elapsed: 0:21:50.\n",
      "  Batch 1,890  of  3,504.    Elapsed: 0:21:57.\n",
      "  Batch 1,900  of  3,504.    Elapsed: 0:22:04.\n",
      "  Batch 1,910  of  3,504.    Elapsed: 0:22:10.\n",
      "  Batch 1,920  of  3,504.    Elapsed: 0:22:17.\n",
      "  Batch 1,930  of  3,504.    Elapsed: 0:22:24.\n",
      "  Batch 1,940  of  3,504.    Elapsed: 0:22:31.\n",
      "  Batch 1,950  of  3,504.    Elapsed: 0:22:38.\n",
      "  Batch 1,960  of  3,504.    Elapsed: 0:22:45.\n",
      "  Batch 1,970  of  3,504.    Elapsed: 0:22:52.\n",
      "  Batch 1,980  of  3,504.    Elapsed: 0:22:59.\n",
      "  Batch 1,990  of  3,504.    Elapsed: 0:23:06.\n",
      "  Batch 2,000  of  3,504.    Elapsed: 0:23:13.\n",
      "  Batch 2,010  of  3,504.    Elapsed: 0:23:20.\n",
      "  Batch 2,020  of  3,504.    Elapsed: 0:23:27.\n",
      "  Batch 2,030  of  3,504.    Elapsed: 0:23:34.\n",
      "  Batch 2,040  of  3,504.    Elapsed: 0:23:41.\n",
      "  Batch 2,050  of  3,504.    Elapsed: 0:23:48.\n",
      "  Batch 2,060  of  3,504.    Elapsed: 0:23:55.\n",
      "  Batch 2,070  of  3,504.    Elapsed: 0:24:02.\n",
      "  Batch 2,080  of  3,504.    Elapsed: 0:24:09.\n",
      "  Batch 2,090  of  3,504.    Elapsed: 0:24:16.\n",
      "  Batch 2,100  of  3,504.    Elapsed: 0:24:23.\n",
      "  Batch 2,110  of  3,504.    Elapsed: 0:24:30.\n",
      "  Batch 2,120  of  3,504.    Elapsed: 0:24:37.\n",
      "  Batch 2,130  of  3,504.    Elapsed: 0:24:44.\n",
      "  Batch 2,140  of  3,504.    Elapsed: 0:24:51.\n",
      "  Batch 2,150  of  3,504.    Elapsed: 0:24:58.\n",
      "  Batch 2,160  of  3,504.    Elapsed: 0:25:05.\n",
      "  Batch 2,170  of  3,504.    Elapsed: 0:25:12.\n",
      "  Batch 2,180  of  3,504.    Elapsed: 0:25:19.\n",
      "  Batch 2,190  of  3,504.    Elapsed: 0:25:26.\n",
      "  Batch 2,200  of  3,504.    Elapsed: 0:25:33.\n",
      "  Batch 2,210  of  3,504.    Elapsed: 0:25:40.\n",
      "  Batch 2,220  of  3,504.    Elapsed: 0:25:46.\n",
      "  Batch 2,230  of  3,504.    Elapsed: 0:25:53.\n",
      "  Batch 2,240  of  3,504.    Elapsed: 0:26:00.\n",
      "  Batch 2,250  of  3,504.    Elapsed: 0:26:07.\n",
      "  Batch 2,260  of  3,504.    Elapsed: 0:26:14.\n",
      "  Batch 2,270  of  3,504.    Elapsed: 0:26:21.\n",
      "  Batch 2,280  of  3,504.    Elapsed: 0:26:28.\n",
      "  Batch 2,290  of  3,504.    Elapsed: 0:26:35.\n",
      "  Batch 2,300  of  3,504.    Elapsed: 0:26:42.\n",
      "  Batch 2,310  of  3,504.    Elapsed: 0:26:49.\n",
      "  Batch 2,320  of  3,504.    Elapsed: 0:26:56.\n",
      "  Batch 2,330  of  3,504.    Elapsed: 0:27:03.\n",
      "  Batch 2,340  of  3,504.    Elapsed: 0:27:10.\n",
      "  Batch 2,350  of  3,504.    Elapsed: 0:27:17.\n",
      "  Batch 2,360  of  3,504.    Elapsed: 0:27:24.\n",
      "  Batch 2,370  of  3,504.    Elapsed: 0:27:31.\n",
      "  Batch 2,380  of  3,504.    Elapsed: 0:27:38.\n",
      "  Batch 2,390  of  3,504.    Elapsed: 0:27:45.\n",
      "  Batch 2,400  of  3,504.    Elapsed: 0:27:52.\n",
      "  Batch 2,410  of  3,504.    Elapsed: 0:27:59.\n",
      "  Batch 2,420  of  3,504.    Elapsed: 0:28:06.\n",
      "  Batch 2,430  of  3,504.    Elapsed: 0:28:13.\n",
      "  Batch 2,440  of  3,504.    Elapsed: 0:28:20.\n",
      "  Batch 2,450  of  3,504.    Elapsed: 0:28:27.\n",
      "  Batch 2,460  of  3,504.    Elapsed: 0:28:34.\n",
      "  Batch 2,470  of  3,504.    Elapsed: 0:28:41.\n",
      "  Batch 2,480  of  3,504.    Elapsed: 0:28:48.\n",
      "  Batch 2,490  of  3,504.    Elapsed: 0:28:54.\n",
      "  Batch 2,500  of  3,504.    Elapsed: 0:29:01.\n",
      "  Batch 2,510  of  3,504.    Elapsed: 0:29:08.\n",
      "  Batch 2,520  of  3,504.    Elapsed: 0:29:15.\n",
      "  Batch 2,530  of  3,504.    Elapsed: 0:29:22.\n",
      "  Batch 2,540  of  3,504.    Elapsed: 0:29:29.\n",
      "  Batch 2,550  of  3,504.    Elapsed: 0:29:36.\n",
      "  Batch 2,560  of  3,504.    Elapsed: 0:29:43.\n",
      "  Batch 2,570  of  3,504.    Elapsed: 0:29:50.\n",
      "  Batch 2,580  of  3,504.    Elapsed: 0:29:57.\n",
      "  Batch 2,590  of  3,504.    Elapsed: 0:30:04.\n",
      "  Batch 2,600  of  3,504.    Elapsed: 0:30:11.\n",
      "  Batch 2,610  of  3,504.    Elapsed: 0:30:18.\n",
      "  Batch 2,620  of  3,504.    Elapsed: 0:30:25.\n",
      "  Batch 2,630  of  3,504.    Elapsed: 0:30:32.\n",
      "  Batch 2,640  of  3,504.    Elapsed: 0:30:39.\n",
      "  Batch 2,650  of  3,504.    Elapsed: 0:30:46.\n",
      "  Batch 2,660  of  3,504.    Elapsed: 0:30:53.\n",
      "  Batch 2,670  of  3,504.    Elapsed: 0:31:00.\n",
      "  Batch 2,680  of  3,504.    Elapsed: 0:31:07.\n",
      "  Batch 2,690  of  3,504.    Elapsed: 0:31:14.\n",
      "  Batch 2,700  of  3,504.    Elapsed: 0:31:21.\n",
      "  Batch 2,710  of  3,504.    Elapsed: 0:31:28.\n",
      "  Batch 2,720  of  3,504.    Elapsed: 0:31:35.\n",
      "  Batch 2,730  of  3,504.    Elapsed: 0:31:42.\n",
      "  Batch 2,740  of  3,504.    Elapsed: 0:31:49.\n",
      "  Batch 2,750  of  3,504.    Elapsed: 0:31:55.\n",
      "  Batch 2,760  of  3,504.    Elapsed: 0:32:02.\n",
      "  Batch 2,770  of  3,504.    Elapsed: 0:32:09.\n",
      "  Batch 2,780  of  3,504.    Elapsed: 0:32:16.\n",
      "  Batch 2,790  of  3,504.    Elapsed: 0:32:23.\n",
      "  Batch 2,800  of  3,504.    Elapsed: 0:32:30.\n",
      "  Batch 2,810  of  3,504.    Elapsed: 0:32:37.\n",
      "  Batch 2,820  of  3,504.    Elapsed: 0:32:44.\n",
      "  Batch 2,830  of  3,504.    Elapsed: 0:32:51.\n",
      "  Batch 2,840  of  3,504.    Elapsed: 0:32:58.\n",
      "  Batch 2,850  of  3,504.    Elapsed: 0:33:05.\n",
      "  Batch 2,860  of  3,504.    Elapsed: 0:33:12.\n",
      "  Batch 2,870  of  3,504.    Elapsed: 0:33:19.\n",
      "  Batch 2,880  of  3,504.    Elapsed: 0:33:26.\n",
      "  Batch 2,890  of  3,504.    Elapsed: 0:33:33.\n",
      "  Batch 2,900  of  3,504.    Elapsed: 0:33:40.\n",
      "  Batch 2,910  of  3,504.    Elapsed: 0:33:47.\n",
      "  Batch 2,920  of  3,504.    Elapsed: 0:33:54.\n",
      "  Batch 2,930  of  3,504.    Elapsed: 0:34:01.\n",
      "  Batch 2,940  of  3,504.    Elapsed: 0:34:08.\n",
      "  Batch 2,950  of  3,504.    Elapsed: 0:34:15.\n",
      "  Batch 2,960  of  3,504.    Elapsed: 0:34:22.\n",
      "  Batch 2,970  of  3,504.    Elapsed: 0:34:29.\n",
      "  Batch 2,980  of  3,504.    Elapsed: 0:34:36.\n",
      "  Batch 2,990  of  3,504.    Elapsed: 0:34:43.\n",
      "  Batch 3,000  of  3,504.    Elapsed: 0:34:50.\n",
      "  Batch 3,010  of  3,504.    Elapsed: 0:34:57.\n",
      "  Batch 3,020  of  3,504.    Elapsed: 0:35:04.\n",
      "  Batch 3,030  of  3,504.    Elapsed: 0:35:10.\n",
      "  Batch 3,040  of  3,504.    Elapsed: 0:35:17.\n",
      "  Batch 3,050  of  3,504.    Elapsed: 0:35:24.\n",
      "  Batch 3,060  of  3,504.    Elapsed: 0:35:31.\n",
      "  Batch 3,070  of  3,504.    Elapsed: 0:35:38.\n",
      "  Batch 3,080  of  3,504.    Elapsed: 0:35:45.\n",
      "  Batch 3,090  of  3,504.    Elapsed: 0:35:52.\n",
      "  Batch 3,100  of  3,504.    Elapsed: 0:35:59.\n",
      "  Batch 3,110  of  3,504.    Elapsed: 0:36:06.\n",
      "  Batch 3,120  of  3,504.    Elapsed: 0:36:13.\n",
      "  Batch 3,130  of  3,504.    Elapsed: 0:36:20.\n",
      "  Batch 3,140  of  3,504.    Elapsed: 0:36:27.\n",
      "  Batch 3,150  of  3,504.    Elapsed: 0:36:34.\n",
      "  Batch 3,160  of  3,504.    Elapsed: 0:36:41.\n",
      "  Batch 3,170  of  3,504.    Elapsed: 0:36:48.\n",
      "  Batch 3,180  of  3,504.    Elapsed: 0:36:55.\n",
      "  Batch 3,190  of  3,504.    Elapsed: 0:37:02.\n",
      "  Batch 3,200  of  3,504.    Elapsed: 0:37:09.\n",
      "  Batch 3,210  of  3,504.    Elapsed: 0:37:16.\n",
      "  Batch 3,220  of  3,504.    Elapsed: 0:37:23.\n",
      "  Batch 3,230  of  3,504.    Elapsed: 0:37:30.\n",
      "  Batch 3,240  of  3,504.    Elapsed: 0:37:37.\n",
      "  Batch 3,250  of  3,504.    Elapsed: 0:37:44.\n",
      "  Batch 3,260  of  3,504.    Elapsed: 0:37:50.\n",
      "  Batch 3,270  of  3,504.    Elapsed: 0:37:57.\n",
      "  Batch 3,280  of  3,504.    Elapsed: 0:38:04.\n",
      "  Batch 3,290  of  3,504.    Elapsed: 0:38:11.\n",
      "  Batch 3,300  of  3,504.    Elapsed: 0:38:18.\n",
      "  Batch 3,310  of  3,504.    Elapsed: 0:38:25.\n",
      "  Batch 3,320  of  3,504.    Elapsed: 0:38:32.\n",
      "  Batch 3,330  of  3,504.    Elapsed: 0:38:39.\n",
      "  Batch 3,340  of  3,504.    Elapsed: 0:38:46.\n",
      "  Batch 3,350  of  3,504.    Elapsed: 0:38:53.\n",
      "  Batch 3,360  of  3,504.    Elapsed: 0:39:00.\n",
      "  Batch 3,370  of  3,504.    Elapsed: 0:39:07.\n",
      "  Batch 3,380  of  3,504.    Elapsed: 0:39:14.\n",
      "  Batch 3,390  of  3,504.    Elapsed: 0:39:21.\n",
      "  Batch 3,400  of  3,504.    Elapsed: 0:39:28.\n",
      "  Batch 3,410  of  3,504.    Elapsed: 0:39:35.\n",
      "  Batch 3,420  of  3,504.    Elapsed: 0:39:42.\n",
      "  Batch 3,430  of  3,504.    Elapsed: 0:39:49.\n",
      "  Batch 3,440  of  3,504.    Elapsed: 0:39:56.\n",
      "  Batch 3,450  of  3,504.    Elapsed: 0:40:03.\n",
      "  Batch 3,460  of  3,504.    Elapsed: 0:40:10.\n",
      "  Batch 3,470  of  3,504.    Elapsed: 0:40:17.\n",
      "  Batch 3,480  of  3,504.    Elapsed: 0:40:24.\n",
      "  Batch 3,490  of  3,504.    Elapsed: 0:40:31.\n",
      "  Batch 3,500  of  3,504.    Elapsed: 0:40:38.\n",
      "\n",
      "  Average training loss generetor: 0.700\n",
      "  Average training loss discriminator: 0.803\n",
      "  Training epcoh took: 0:40:40\n",
      "\n",
      "Running Test...\n",
      "all_preds\n",
      "[5 1 1 ... 4 4 4]\n",
      "all_labels\n",
      "[3 1 1 ... 3 5 4]\n",
      "  Accuracy: 0.539\n",
      "  Test Loss: 1.890\n",
      "  Test took: 0:00:12\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, num_train_epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    tr_g_loss = 0\n",
    "    tr_d_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    transformer.train() \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every print_each_n_step batches.\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_label_mask = batch[3].to(device)\n",
    "\n",
    "        real_batch_size = b_input_ids.shape[0]\n",
    "     \n",
    "        # Encode real data in the Transformer\n",
    "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "        hidden_states = model_outputs[-1]\n",
    "        \n",
    "        # Generate fake data that should have the same distribution of the ones\n",
    "        # encoded by the transformer. \n",
    "        # First noisy input are used in input to the Generator\n",
    "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "        # Gnerate Fake data\n",
    "        gen_rep = generator(noise)\n",
    "\n",
    "        # Generate the output of the Discriminator for real and fake data.\n",
    "        # First, we put together the output of the tranformer and the generator\n",
    "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "        # Then, we select the output of the disciminator\n",
    "        features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "        # Finally, we separate the discriminator's output for the real and fake\n",
    "        # data\n",
    "        features_list = torch.split(features, real_batch_size)\n",
    "        D_real_features = features_list[0]\n",
    "        D_fake_features = features_list[1]\n",
    "      \n",
    "        logits_list = torch.split(logits, real_batch_size)\n",
    "        D_real_logits = logits_list[0]\n",
    "        D_fake_logits = logits_list[1]\n",
    "        \n",
    "        probs_list = torch.split(probs, real_batch_size)\n",
    "        D_real_probs = probs_list[0]\n",
    "        D_fake_probs = probs_list[1]\n",
    "\n",
    "        #---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        #---------------------------------\n",
    "        # Generator's LOSS estimation\n",
    "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "        g_loss = g_loss_d + g_feat_reg\n",
    "  \n",
    "        # Disciminator's LOSS estimation\n",
    "        logits = D_real_logits[:,0:-1]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # The discriminator provides an output for labeled and unlabeled real data\n",
    "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "        # It may be the case that a batch does not contain labeled examples, \n",
    "        # so the \"supervised loss\" in this case is not evaluated\n",
    "        if labeled_example_count == 0:\n",
    "          D_L_Supervised = 0\n",
    "        else:\n",
    "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "                 \n",
    "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "        #---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        #---------------------------------\n",
    "        # Avoid gradient accumulation\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate weigth updates\n",
    "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        d_loss.backward() \n",
    "        \n",
    "        # Apply modifications\n",
    "        gen_optimizer.step()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # A detail log of the individual losses\n",
    "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "        #             g_loss_d, g_feat_reg))\n",
    "\n",
    "        # Save the losses to print them later\n",
    "        tr_g_loss += g_loss.item()\n",
    "        tr_d_loss += d_loss.item()\n",
    "\n",
    "        # Update the learning rate with the scheduler\n",
    "        if apply_scheduler:\n",
    "          scheduler_d.step()\n",
    "          scheduler_g.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #     TEST ON THE EVALUATION DATASET\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our test set.\n",
    "    print(\"\")\n",
    "    print(\"Running Test...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    transformer.eval() #maybe redundant\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_test_accuracy = 0\n",
    "   \n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    #loss\n",
    "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            _, logits, probs = discriminator(hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = logits[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "            \n",
    "        # Accumulate the predictions and the input labels\n",
    "        _, preds = torch.max(filtered_logits, 1)\n",
    "        all_preds += preds.detach().cpu()\n",
    "        all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    print('all_preds')\n",
    "    print(all_preds)\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    print('all_labels')\n",
    "    print(all_labels_ids)\n",
    "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    avg_test_loss = avg_test_loss.item()\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    test_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss generator': avg_train_loss_g,\n",
    "            'Training Loss discriminator': avg_train_loss_d,\n",
    "            'Valid. Loss': avg_test_loss,\n",
    "            'Valid. Accur.': test_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Test Time': test_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dDm9NProRB4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'Training Loss generator': 0.7063582741855282, 'Training Loss discriminator': 1.5937957020314861, 'Valid. Loss': 1.28456449508667, 'Valid. Accur.': 0.557, 'Training Time': '0:40:40', 'Test Time': '0:00:12'}\n",
      "{'epoch': 2, 'Training Loss generator': 0.7035861017361079, 'Training Loss discriminator': 1.0648218038827861, 'Valid. Loss': 1.7748814821243286, 'Valid. Accur.': 0.5364, 'Training Time': '0:40:40', 'Test Time': '0:00:12'}\n",
      "{'epoch': 3, 'Training Loss generator': 0.701852003954453, 'Training Loss discriminator': 0.8818188694105845, 'Valid. Loss': 1.765709400177002, 'Valid. Accur.': 0.5616, 'Training Time': '0:40:39', 'Test Time': '0:00:12'}\n",
      "{'epoch': 4, 'Training Loss generator': 0.7007282321855902, 'Training Loss discriminator': 0.8341780413775683, 'Valid. Loss': 2.238259792327881, 'Valid. Accur.': 0.538, 'Training Time': '0:40:39', 'Test Time': '0:00:12'}\n",
      "{'epoch': 5, 'Training Loss generator': 0.7003401024904969, 'Training Loss discriminator': 0.8031283940728668, 'Valid. Loss': 1.8901718854904175, 'Valid. Accur.': 0.5394, 'Training Time': '0:40:40', 'Test Time': '0:00:12'}\n",
      "\n",
      "Training complete!\n",
      "Total training took 3:24:20 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "for stat in training_stats:\n",
    "  print(stat)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icYwe1wXgetE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "““GANBERT_5%_Yelp.ipynb”",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023e75b78bc0458f812801638946d7ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_458b776fbffd404eb69aab9d07f9be53",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_088e74d10cd34b36b3743c5bac1e73fa",
      "value": 2
     }
    },
    "06e9ed4ad90f46a080bedde4f270269a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fef9a563ac1347b393985eb69cf8f634",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f56206a1bc74620903cea2230e6ca54",
      "value": 5000
     }
    },
    "088e74d10cd34b36b3743c5bac1e73fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1099803863ad4202a21fa5731cbccc17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "158a2f31483a4f668ee1aad205c4d6ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a07fdd1e59a4fadb0e5ae24ddc5aa58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de4ef3bf5467485fb91e13fb492f5e4a",
       "IPY_MODEL_e9ac3edb2fa84247993d39b12b66d0a9",
       "IPY_MODEL_3faab90fa4ad4daaa6c15214ee6c8648"
      ],
      "layout": "IPY_MODEL_577e5d921add48f39e230d6e20afd9ac"
     }
    },
    "1d3249030cc941c0933fca6aeacccb23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f56206a1bc74620903cea2230e6ca54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "28ddac0fc57f40a893c6c25d363d3b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bdad124edbf4f149f803e01b3cb45ff",
       "IPY_MODEL_023e75b78bc0458f812801638946d7ca",
       "IPY_MODEL_f3777bf57258424c9bae0f180e38fca9"
      ],
      "layout": "IPY_MODEL_aa248bd910fe47c18fe15bec280d2806"
     }
    },
    "330d42120384471192fcb9afaaa49e4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37be672361f94deda5d94b093a7c4a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a278779bf114a5d9c01b46d8e3a7084": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5786bed9383410680a176c870145a36",
       "IPY_MODEL_06e9ed4ad90f46a080bedde4f270269a",
       "IPY_MODEL_5a9cf9a79f904592a328a77ab573c8e0"
      ],
      "layout": "IPY_MODEL_3a84d2e1c7ce4d09b4ec694989469a70"
     }
    },
    "3a84d2e1c7ce4d09b4ec694989469a70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3faab90fa4ad4daaa6c15214ee6c8648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2395d83dc22422b8643f9c0e8fc741b",
      "placeholder": "​",
      "style": "IPY_MODEL_b1ca3fa910bb40db86094c1cd78bb273",
      "value": " 9750/9750 [00:03&lt;00:00, 2530.48it/s]"
     }
    },
    "458b776fbffd404eb69aab9d07f9be53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f6d01f1fb4d4631a91358f47f5b1507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1d0181e365642debacc42961d6f70c8",
       "IPY_MODEL_bcbd78b9686c425a8eee96fa628bf752",
       "IPY_MODEL_57deaebe133c4fee9e46f9e31ff739d0"
      ],
      "layout": "IPY_MODEL_6c0c45edfef146fca0eec9f700a0f638"
     }
    },
    "517dc9f1c1aa47ada04e459a3361a86d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "576590ab88514315a8c698fe178a84b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "577e5d921add48f39e230d6e20afd9ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57deaebe133c4fee9e46f9e31ff739d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af8962d0b3bf4a54ae27b388c8d7d708",
      "placeholder": "​",
      "style": "IPY_MODEL_517dc9f1c1aa47ada04e459a3361a86d",
      "value": " 185250/185250 [01:13&lt;00:00, 2406.41it/s]"
     }
    },
    "5a9cf9a79f904592a328a77ab573c8e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96176dc8861849edb248dd6b3a451955",
      "placeholder": "​",
      "style": "IPY_MODEL_b57fd3bce49f4845addea1832dca1676",
      "value": " 5000/5000 [00:02&lt;00:00, 2431.46it/s]"
     }
    },
    "6c0c45edfef146fca0eec9f700a0f638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bdad124edbf4f149f803e01b3cb45ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_330d42120384471192fcb9afaaa49e4f",
      "placeholder": "​",
      "style": "IPY_MODEL_8d9d0143cb304eb596971adcc7e392e8",
      "value": "100%"
     }
    },
    "841a888184524f1889c6228a5111150f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d9d0143cb304eb596971adcc7e392e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96176dc8861849edb248dd6b3a451955": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa248bd910fe47c18fe15bec280d2806": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8962d0b3bf4a54ae27b388c8d7d708": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1ca3fa910bb40db86094c1cd78bb273": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1d0181e365642debacc42961d6f70c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc8724ca9e9a484197a90f47fc3999d2",
      "placeholder": "​",
      "style": "IPY_MODEL_1099803863ad4202a21fa5731cbccc17",
      "value": "100%"
     }
    },
    "b57fd3bce49f4845addea1832dca1676": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb156789ac354f7ca2e463302e81609e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc8724ca9e9a484197a90f47fc3999d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcbd78b9686c425a8eee96fa628bf752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb45fb4430074b03b02e2608ba56c24d",
      "max": 185250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_841a888184524f1889c6228a5111150f",
      "value": 185250
     }
    },
    "c5786bed9383410680a176c870145a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd79f3c885ba47d59c952428bf3541d7",
      "placeholder": "​",
      "style": "IPY_MODEL_bb156789ac354f7ca2e463302e81609e",
      "value": "100%"
     }
    },
    "c98d570667a2487da8e9c88955852c22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2395d83dc22422b8643f9c0e8fc741b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de4ef3bf5467485fb91e13fb492f5e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_576590ab88514315a8c698fe178a84b5",
      "placeholder": "​",
      "style": "IPY_MODEL_37be672361f94deda5d94b093a7c4a77",
      "value": "100%"
     }
    },
    "e9ac3edb2fa84247993d39b12b66d0a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c98d570667a2487da8e9c88955852c22",
      "max": 9750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_158a2f31483a4f668ee1aad205c4d6ac",
      "value": 9750
     }
    },
    "f3777bf57258424c9bae0f180e38fca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d3249030cc941c0933fca6aeacccb23",
      "placeholder": "​",
      "style": "IPY_MODEL_f492bb406b4a455192e5348345e576d1",
      "value": " 2/2 [00:00&lt;00:00, 57.71it/s]"
     }
    },
    "f492bb406b4a455192e5348345e576d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb45fb4430074b03b02e2608ba56c24d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd79f3c885ba47d59c952428bf3541d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fef9a563ac1347b393985eb69cf8f634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
