# Semi-Supervised-Text-Classification-with-Generative-Adversarial-Nets

# Background
Text classification is an extremely important information for companies. Thanks to the invention of transformer based model like Bidirectional Encoder Representations from Transformers(BERT) and Gener-ative Pre-trained Transformer(GPT),  has largely improved the performance of text classification compared to original Recurrent Neural Network. In real business scenarios, most of good performances come from millions of annotated instances , but usually it’s impossible for some companies getting enough annotated data because it needs too much time and labour. 

Previous research has proposed GAN-BERT, which combined Generative Adversarial Nets(GAN)and BERT for training a semi-supervised model.It shows GAN could improve the performance of BERT in semi-supervised tasks. In this research, we propose the GAN-GPT2 model which combined GAN and GPT, and compare it with BERT,GPT2,GAN-BERT in semi-supervised experiments. 

# Methodology
For each model, we test it on three datasets: Amazon Review dataset, Yelp Review dataset and Financial Review dataset, with different proportion of annotated examples. Experiment result shows GAN-BERT and GAN-GPT2 only need less annotated examples to get a good performance. Compared to the result of BERT and GPT training with all annotated examples, GAN-BERT and GAN-GPT2 only need 5%-10% annotated examples to get the same performance. Especially, our result shows GAN could largely improve GPT2’s performance when facing less than 10% annotated data.

# Model
GPT2 uses the decoder part of Transformer. So we change the model to make sure it could also work with the SS-GAN model. First, we change the padding method from left padding which was used in BERT to right padding. Padding is a skill when we have different sequence length in our dataset. We add the a special token in the sentence to make sure each sentence has the same length before training, which means our dimension of each input is the same. In BERT, they use the first token as the representation of a sentence embedding so the padding method is right padding which means it add special
token to the end of the sentence. But GPT2 is different. GPT2 use the last token as the representation of a sentence embedding so the padding method is left padding which means it add special token to the beginning of the sentence.

The GPT2 model we use has 12 hidden layers and the last hidden layers extract all of the information from 10 input. Under consideration of our padding method, we select the last token in the last hidden layer as the final representation of a sentence. Then we concatenate the output of hidden layer and the fake data generated by the generator G as the input of the discriminator D. Finally, the discriminator use the combined input to
predict. The modified structure of GAN-GPT2 is shown in figure below.
![My Image](imagefolder/GAN-GPT2.jpg)
# Result
Our test result was shown in the following figures.
![My Image](imagefolder/Amazon_Result.jpg)

![My Image](imagefolder/Yelp_Result.jpg)

![My Image](imagefolder/Financial_Review.jpg)

# Conclusion

